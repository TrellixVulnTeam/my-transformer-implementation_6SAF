{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torchtext\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertEqual(t1, t2, message=\"\"):\n",
    "    assert torch.eq(t1, t2).all()\n",
    "    print(\"Checking: \", message, \"     ...OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_equal(t1, t2):\n",
    "    nonequal_elements = int((t1 != t2).sum())\n",
    "    equal_elements = int((t1 == t2).sum())\n",
    "    equal_percentage = equal_elements / (nonequal_elements + equal_elements)\n",
    "    return equal_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_head corresponds to d_k in the paper\n",
    "def scaled_dot_product_attention(value, key, query, dropout=0.0):\n",
    "    \"\"\"\n",
    "    Shape:\n",
    "    - Inputs:\n",
    "    - query: `(..., T, N * H, E / H)`\n",
    "    - key: `(..., S, N * H, E / H)`\n",
    "    - value: `(..., S, N * H, E /H)`\n",
    "    \n",
    "    where E = d_model, E/H = d_head\n",
    "    \n",
    "    - Outputs:\n",
    "    - `(..., T, N * H, E / H)`, `(N * H, T, S)`\n",
    "    \"\"\"\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"The d_head of query, key, value must be equal.\"\n",
    "    S, T, N_H, d_head = key.shape[-3], query.shape[-3], query.shape[-2], query.shape[-1]\n",
    "    \n",
    "    query, key, value = query.transpose(-2, -3), key.transpose(-2, -3), value.transpose(-2, -3)\n",
    "\n",
    "    # calculates attention weights\n",
    "    query = query * (float(d_head) ** -0.5)     \n",
    "    attention_weights = torch.matmul(query, key.transpose(-2,-1))\n",
    "    attention_weights = torch.nn.functional.softmax(attention_weights, dim=-1)\n",
    "    attention_weights = torch.nn.functional.dropout(attention_weights, p=dropout)\n",
    "    assert attention_weights.shape == (N_H, T, S), \"attention_weights should be shape (N * H, T, S)\"\n",
    "\n",
    "    attention_output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return attention_output.transpose(-3, -2), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:  attention_weights are expected?      ...OK!\n",
      "Checking:  attention_output is expected?      ...OK!\n"
     ]
    }
   ],
   "source": [
    "def test_sdp():\n",
    "    q = torch.randn(25, 256, 3)\n",
    "    k = v = torch.randn(21, 256, 3)\n",
    "    \n",
    "    # call torchtext's SDP\n",
    "    SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)\n",
    "    torch.manual_seed(42)\n",
    "    expected_attn_output, expected_attn_weights = SDP(q, k, v)\n",
    "    \n",
    "    # call our SDP\n",
    "    torch.manual_seed(42)    \n",
    "    attn_output, attn_weights = scaled_dot_product_attention(v, k, q, dropout=0.1) \n",
    "    \n",
    "    assert attn_weights.shape == expected_attn_weights.shape\n",
    "    assertEqual(attn_weights, expected_attn_weights, message = \"attention_weights are expected?\")\n",
    "    assert attn_output.shape == expected_attn_output.shape, \"attn_output.shape is {0} whereas expected_output.shape is {1}\".format(attn_output.shape, expected_attn_output.shape) \n",
    "    assertEqual(attn_output, expected_attn_output, message = \"attention_output is expected?\")\n",
    "    \n",
    "test_sdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(torch.nn.Module):\n",
    "    def __init__(self, d_model, W_v, W_k, W_q):\n",
    "        super().__init__()\n",
    "        self.W_v, self.W_k, self.W_q = W_v, W_k, W_q   \n",
    "        \n",
    "    def forward(self, attended, attending):\n",
    "        # input dimension is (sentence_len, batch_size, d_model)\n",
    "        value = self.W_v(attended)\n",
    "        key = self.W_k(attended)\n",
    "        query = self.W_q(attending)\n",
    "\n",
    "        return value, key, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:  query is expected?      ...OK!\n",
      "Checking:  key is expected?      ...OK!\n",
      "Checking:  value is expected?      ...OK!\n"
     ]
    }
   ],
   "source": [
    "def test_projection():\n",
    "    from torchtext.nn import InProjContainer\n",
    "\n",
    "    d_model, batch_size = 10, 64\n",
    "    q = torch.rand((5, batch_size, d_model))\n",
    "    k = v = torch.rand((6, batch_size, d_model))\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    l1 = torch.nn.Linear(d_model, d_model)\n",
    "    l2 = l3 = copy.deepcopy(l1)\n",
    "    \n",
    "    # call torchtext's InProjContainer class\n",
    "    in_proj_container = InProjContainer(l1, l2, l3)\n",
    "    expected_query, expected_key, expected_value = in_proj_container(q, k, v)\n",
    "\n",
    "    # call our Projection class\n",
    "    projection = Projection(d_model, l1, l2, l3)\n",
    "    value, key, query = projection(k, q)\n",
    "\n",
    "    assert expected_query.shape == query.shape\n",
    "    assert expected_key.shape == key.shape\n",
    "    assert expected_value.shape == value.shape\n",
    "\n",
    "    assertEqual(expected_query, query, message = \"query is expected?\")\n",
    "    assertEqual(expected_key, key, message = \"key is expected?\")\n",
    "    assertEqual(expected_value, value, message = \"value is expected?\")\n",
    "\n",
    "test_projection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, nheads, d_model, W_v, W_k, W_q, W_o, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.projection = Projection(d_model, W_v, W_k, W_q)\n",
    "        self.W_v, self.W_k, self.W_q, self.W_o = W_v, W_k, W_q, W_o\n",
    "        \n",
    "    def forward(self, attended, attending, mask=None):\n",
    "        '''\n",
    "        Shape:\n",
    "        - Inputs:\n",
    "        - attending: :math:`(..., T, N, d_model)`\n",
    "        - attended: :math:`(..., S, N, d_model)`\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(..., T, N, d_model)`\n",
    "        - attn_output_weights: :math:`(N * H, T, S)`\n",
    "        '''\n",
    "        # checks dimensions & assigns variables\n",
    "        assert attending.shape[-1] == attended.shape[-1], \"attending & attended should have the same d_model\"\n",
    "        d_model = attending.shape[-1]\n",
    "        assert d_model % self.nheads == 0, \"d_model should be divisible by number of heads\"\n",
    "        self.d_k = d_model // self.nheads  \n",
    "        assert attending.shape[-2] == attended.shape[-2], \"attending & attended should have the same batch size\"\n",
    "        self.batch_size = attending.shape[-2]\n",
    "        \n",
    "        # projects attended and attending tensors to v, k, q\n",
    "        value, key, query = self.projection(attended, attending)\n",
    "        value, key, query = self.reshape_into_nheads(value, self.nheads, self.d_k), self.reshape_into_nheads(key, self.nheads, self.d_k), self.reshape_into_nheads(query, self.nheads, self.d_k)\n",
    "                \n",
    "        # forward multi-heads through SDP\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(value, key, query)\n",
    "        assert attn_output.shape == (attending.shape[-3], self.batch_size * self.nheads, self.d_k), \"attn_output's shape from SDP should be (..., T, N * H, E / H)\"\n",
    "\n",
    "        # concats multi-heads and forward through final layer\n",
    "        attn_output = self.reshape_into_nheads(attn_output, 1, d_model)\n",
    "        attn_output = self.W_o(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "    def reshape_into_nheads(self, x, nheads, last_dim):\n",
    "        return x.reshape(-1, self.batch_size * nheads, last_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6359375\n",
      "1.0\n",
      "Checking:  attn_output expected?      ...OK!\n",
      "Checking:  attn_weights expected?      ...OK!\n"
     ]
    }
   ],
   "source": [
    "def test_multihead_attention():\n",
    "    from torchtext.nn import InProjContainer, MultiheadAttentionContainer, ScaledDotProduct\n",
    "    from torch.nn.functional import multi_head_attention_forward as mha_forward\n",
    "\n",
    "    d_model, num_heads, bsz = 10, 5, 64\n",
    "    query = torch.rand((21, bsz, d_model))\n",
    "    key = value = torch.rand((16, bsz, d_model))\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    l1 = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "    l2 = l3 = l4 = copy.deepcopy(l1)\n",
    "    \n",
    "    # call torchtext's MHA\n",
    "    in_proj_container = InProjContainer(l1, l2, l3)\n",
    "    torchtext_mha = MultiheadAttentionContainer(num_heads, \n",
    "                                      in_proj_container, \n",
    "                                      ScaledDotProduct(),\n",
    "                                      l4)\n",
    "    torchtext_attn_output, torchtext_attn_weights = torchtext_mha(query, key, value)\n",
    "    \n",
    "    # call torch's MHA\n",
    "    in_proj_weight = torch.cat([torchtext_mha.in_proj_container.query_proj.weight,\n",
    "                                    torchtext_mha.in_proj_container.key_proj.weight,\n",
    "                                    torchtext_mha.in_proj_container.value_proj.weight])\n",
    "    torch_attn_output, torch_attn_weights = mha_forward(query, key, value,\n",
    "                            d_model, num_heads,\n",
    "                            in_proj_weight, None,\n",
    "                            None, None,\n",
    "                            False, 0.0,\n",
    "                            torchtext_mha.out_proj.weight, None)\n",
    "    \n",
    "    print(get_percentage_equal(torch_attn_output, torchtext_attn_output))\n",
    "#     assertEqual(torch_attn_output, torchtext_attn_output)\n",
    "    \n",
    "    # call our MHA\n",
    "    my_mha = MultiheadAttention(num_heads, d_model, l1, l2, l3, l4, dropout = 0.0)\n",
    "    attn_output, attn_weights = my_mha(key, query)\n",
    "    \n",
    "    print(get_percentage_equal(torchtext_attn_output, attn_output))\n",
    "    \n",
    "    assert torchtext_attn_output.shape == attn_output.shape\n",
    "    assert torchtext_attn_weights.shape == attn_weights.shape \n",
    "    assertEqual(torchtext_attn_output, attn_output, message = \"attn_output expected?\")\n",
    "    assertEqual(torchtext_attn_weights, attn_weights, message = \"attn_weights expected?\")\n",
    "    \n",
    "test_multihead_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
       "\n",
       "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
       "\n",
       "Args:\n",
       "    in_features: size of each input sample\n",
       "    out_features: size of each output sample\n",
       "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
       "        Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
       "      additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
       "    - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
       "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
       "\n",
       "Attributes:\n",
       "    weight: the learnable weights of the module of shape\n",
       "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
       "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
       "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
       "            If :attr:`bias` is ``True``, the values are initialized from\n",
       "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> m = nn.Linear(20, 30)\n",
       "    >>> input = torch.randn(128, 20)\n",
       "    >>> output = m(input)\n",
       "    >>> print(output.size())\n",
       "    torch.Size([128, 30])\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.9/site-packages/torch/nn/modules/linear.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     _LinearWithBias, Linear\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.nn import Dropout, LayerNorm, Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = Linear(d_model, d_ff)\n",
    "        self.linear2 = Linear(d_ff, d_model)\n",
    "        self.dropout = Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.nn.functional.relu(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, nheads, d_model, d_ff = 2048, dropout = 0.1, custom_encoder_layer = None):\n",
    "        super().__init__()\n",
    "        # sublayer1\n",
    "        self.attention = MultiheadAttention(nheads, d_model, dropout = dropout) \n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        # sublayer2\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout = dropout) \n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        \n",
    "        # custom shit for testing\n",
    "        if custom_encoder_layer != None:\n",
    "            self.custom_mha_result = custom_encoder_layer.mha_result\n",
    "            self.custom_feedforward_result = custom_encoder_layer.feedforward_result\n",
    "            self.custom_sublayer1 = custom_encoder_layer.sublayer1\n",
    "            self.custom_sublayer1_normalized = custom_encoder_layer.sublayer1_normalized\n",
    "            self.custom_sublayer2_normalized = custom_encoder_layer.sublayer2_normalized\n",
    "            \n",
    "        \n",
    "    def forward(self, src):\n",
    "        # sublayer1 \n",
    "        # res layer\n",
    "        torch.manual_seed(42)\n",
    "        if self.custom_mha_result != None:\n",
    "            src = self.dropout1(self.custom_mha_result) + src \n",
    "        else:\n",
    "            src = self.dropout1(self.attention(src, src)[0]) + src \n",
    "        # normalize\n",
    "        src = self.norm1(src) \n",
    "        print(\"src vs. sublayer1_normalized: \", get_percentage_equal(src, self.custom_sublayer1_normalized))\n",
    "        assertEqual(src, self.custom_sublayer1_normalized,  \"src vs. sublayer1_normalized\")\n",
    "\n",
    "        # sublayer2\n",
    "        # res layer\n",
    "        if self.custom_feedforward_result != None:\n",
    "            torch.manual_seed(42)\n",
    "            src = self.dropout2(self.custom_feedforward_result) + src\n",
    "        else:\n",
    "            src = self.dropout2(self.feed_forward(src)) + src \n",
    "        # normalize\n",
    "        src = self.norm2(src) # normalize\n",
    "        assertEqual(src, self.custom_sublayer2_normalized,  \"src vs. sublayer2_normalized\")\n",
    "        \n",
    "        \n",
    "        return src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "src_data = torch.rand(10, 32, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8, dropout=0.1)\n",
    "out = encoder_layer(src_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'TransformerEncoderLayer' object has no attribute 'mha_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-651296cd796d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0massertEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my encoder layer output is same as expected?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest_encoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-651296cd796d>\u001b[0m in \u001b[0;36mtest_encoder_layer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# my encoder layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmy_encoder_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_encoder_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmy_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_encoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a21606cdc974>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nheads, d_model, d_ff, dropout, custom_encoder_layer)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# custom shit for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcustom_encoder_layer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_mha_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_encoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_feedforward_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_encoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_sublayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_encoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    779\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'TransformerEncoderLayer' object has no attribute 'mha_result'"
     ]
    }
   ],
   "source": [
    "def test_encoder_layer():\n",
    "    # expected encoder layer\n",
    "    encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8, dropout=0.1)\n",
    "    out = encoder_layer(src_data)\n",
    "    # my encoder layer\n",
    "    my_encoder_layer = EncoderLayer(d_model=512, nheads=8, dropout=0.1, custom_encoder_layer = encoder_layer)\n",
    "    my_out = my_encoder_layer(src_data)\n",
    "\n",
    "    assert my_out.shape == out.shape\n",
    "    print(\"percentage my output is similar to the expected\", get_percentage_equal(my_out, out))\n",
    "    assertEqual(my_out, out, \"my encoder layer output is same as expected?\")\n",
    "    \n",
    "test_encoder_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
