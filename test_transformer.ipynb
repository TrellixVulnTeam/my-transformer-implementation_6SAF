{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertEqual(t1, t2, message):\n",
    "    assert torch.eq(t1, t2).all()\n",
    "    print(\"Checking: \", message, \"     ...OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_head corresponds to d_k in the paper\n",
    "def scaled_dot_product_attention(value, key, query, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Shape:\n",
    "    - query: `(..., T, N * H, E / H)`\n",
    "    - key: `(..., S, N * H, E / H)`\n",
    "    - value: `(..., S, N * H, E /H)`\n",
    "    \n",
    "    where E = d_model, E/H = d_head\n",
    "    \"\"\"\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"The d_head of query, key, value must be equal.\"\n",
    "    S, T, N_H, d_head = key.shape[-3], query.shape[-3], query.shape[-2], query.shape[-1]\n",
    "    \n",
    "    query, key, value = query.transpose(-2, -3), key.transpose(-2, -3), value.transpose(-2, -3)\n",
    "\n",
    "    # calculates attention weights\n",
    "    query = query * (float(d_head) ** -0.5)     \n",
    "    attention_weights = torch.matmul(query, key.transpose(-2,-1))\n",
    "    attention_weights = torch.nn.functional.softmax(attention_weights, dim=-1)\n",
    "    attention_weights = torch.nn.functional.dropout(attention_weights, p=dropout)\n",
    "    assert attention_weights.shape == (N_H, T, S), \"attention_weights should be shape (N * H, T, S)\"\n",
    "\n",
    "    attention_output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return attention_output.transpose(-3, -2), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:  attention_weights are expected?      ...OK!\n",
      "Checking:  attention_output is expected?      ...OK!\n"
     ]
    }
   ],
   "source": [
    "def test_sdp():\n",
    "    SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)\n",
    "    q = torch.randn(25, 256, 3)\n",
    "    k = v = torch.randn(21, 256, 3)\n",
    "    expected_attn_output, expected_attn_weights = SDP(q, k, v)\n",
    "    torch.manual_seed(42)    \n",
    "    attn_output, attn_weights = scaled_dot_product_attention(v, k, q) \n",
    "    \n",
    "    assert attn_weights.shape == expected_attn_weights.shape\n",
    "    assertEqual(attn_weights, expected_attn_weights, \"attention_weights are expected?\")\n",
    "    assert attn_output.shape == expected_attn_output.shape, \"attn_output.shape is {0} whereas expected_output.shape is {1}\".format(attn_output.shape, expected_attn_output.shape) \n",
    "    assertEqual(attn_output, expected_attn_output, \"attention_output is expected?\")\n",
    "    \n",
    "test_sdp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
