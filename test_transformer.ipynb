{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertEqual(t1, t2, message=\"\"):\n",
    "    assert torch.eq(t1, t2).all()\n",
    "    print(\"Checking: \", message, \"     ...OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_head corresponds to d_k in the paper\n",
    "def scaled_dot_product_attention(value, key, query, dropout=0.0):\n",
    "    \"\"\"\n",
    "    Shape:\n",
    "    - Inputs:\n",
    "    - query: `(..., T, N * H, E / H)`\n",
    "    - key: `(..., S, N * H, E / H)`\n",
    "    - value: `(..., S, N * H, E /H)`\n",
    "    \n",
    "    where E = d_model, E/H = d_head\n",
    "    \n",
    "    - Outputs:\n",
    "    - `(..., T, N * H, E / H)`, `(N * H, T, S)`\n",
    "    \"\"\"\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"The d_head of query, key, value must be equal.\"\n",
    "    S, T, N_H, d_head = key.shape[-3], query.shape[-3], query.shape[-2], query.shape[-1]\n",
    "    \n",
    "    query, key, value = query.transpose(-2, -3), key.transpose(-2, -3), value.transpose(-2, -3)\n",
    "\n",
    "    # calculates attention weights\n",
    "    query = query * (float(d_head) ** -0.5)     \n",
    "    attention_weights = torch.matmul(query, key.transpose(-2,-1))\n",
    "    attention_weights = torch.nn.functional.softmax(attention_weights, dim=-1)\n",
    "    attention_weights = torch.nn.functional.dropout(attention_weights, p=dropout)\n",
    "    assert attention_weights.shape == (N_H, T, S), \"attention_weights should be shape (N * H, T, S)\"\n",
    "\n",
    "    attention_output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return attention_output.transpose(-3, -2), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:  attention_weights are expected?      ...OK!\n",
      "Checking:  attention_output is expected?      ...OK!\n"
     ]
    }
   ],
   "source": [
    "def test_sdp():\n",
    "    q = torch.randn(25, 256, 3)\n",
    "    k = v = torch.randn(21, 256, 3)\n",
    "    \n",
    "    # call torchtext's SDP\n",
    "    SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)\n",
    "    expected_attn_output, expected_attn_weights = SDP(q, k, v)\n",
    "    \n",
    "    # call our SDP\n",
    "    torch.manual_seed(42)    \n",
    "    attn_output, attn_weights = scaled_dot_product_attention(v, k, q, dropout=0.1) \n",
    "    \n",
    "    assert attn_weights.shape == expected_attn_weights.shape\n",
    "    assertEqual(attn_weights, expected_attn_weights, message = \"attention_weights are expected?\")\n",
    "    assert attn_output.shape == expected_attn_output.shape, \"attn_output.shape is {0} whereas expected_output.shape is {1}\".format(attn_output.shape, expected_attn_output.shape) \n",
    "    assertEqual(attn_output, expected_attn_output, message = \"attention_output is expected?\")\n",
    "    \n",
    "test_sdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.linears = _get_clones(linear, 3)\n",
    "        \n",
    "        \n",
    "    def forward(self, attended, attending):\n",
    "        # input dimension is (sentence_len, batch_size, d_model)\n",
    "        W_v, W_k, W_q = self.linears\n",
    "\n",
    "        # forward\n",
    "        value = W_v(attended)\n",
    "        key = W_k(attended)\n",
    "        query = W_q(attending)\n",
    "\n",
    "        return value, key, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:  query is expected?      ...OK!\n",
      "Checking:  key is expected?      ...OK!\n",
      "Checking:  key is expected?      ...OK!\n"
     ]
    }
   ],
   "source": [
    "def test_projection():\n",
    "    from torchtext.nn import InProjContainer\n",
    "\n",
    "    d_model, batch_size = 10, 64\n",
    "    q = torch.rand((5, batch_size, d_model))\n",
    "    k = v = torch.rand((6, batch_size, d_model))\n",
    "\n",
    "    # call torchtext's InProjContainer class\n",
    "    torch.manual_seed(42)\n",
    "    l1 = torch.nn.Linear(d_model, d_model)\n",
    "    torch.manual_seed(42)\n",
    "    l2 = torch.nn.Linear(d_model, d_model)\n",
    "    torch.manual_seed(42)\n",
    "    l3 = torch.nn.Linear(d_model, d_model)\n",
    "    in_proj_container = InProjContainer(l1, l2, l3)\n",
    "    expected_query, expected_key, expected_value = in_proj_container(q, k, v)\n",
    "\n",
    "    # call our Projection class\n",
    "    projection = Projection(d_model)\n",
    "    value, key, query = projection(k, q)\n",
    "\n",
    "    assert expected_query.shape == query.shape\n",
    "    assert expected_key.shape == key.shape\n",
    "    assert expected_value.shape == value.shape\n",
    "\n",
    "    assertEqual(expected_query, query, message = \"query is expected?\")\n",
    "    assertEqual(expected_key, key, message = \"key is expected?\")\n",
    "    assertEqual(expected_value, value, message = \"key is expected?\")\n",
    "\n",
    "test_projection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, nheads, d_model):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.projection = Projection(d_model)\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        self.W_o = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, attended, attending, mask=None):\n",
    "        '''\n",
    "        Shape:\n",
    "        - Inputs:\n",
    "        - attending: :math:`(..., T, N, d_model)`\n",
    "        - attended: :math:`(..., S, N, d_model)`\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(..., T, N, d_model)`\n",
    "        - attn_output_weights: :math:`(N * H, T, S)`\n",
    "        '''\n",
    "        # checks dimensions & assigns variables\n",
    "        assert attending.shape[-1] == attended.shape[-1], \"attending & attended should have the same d_model\"\n",
    "        d_model = attending.shape[-1]\n",
    "        assert d_model % self.nheads == 0, \"d_model should be divisible by number of heads\"\n",
    "        self.d_k = d_model // self.nheads  \n",
    "        assert attending.shape[-2] == attended.shape[-2], \"attending & attended should have the same batch size\"\n",
    "        self.batch_size = attending.shape[-2]\n",
    "        \n",
    "        # projects attended and attending tensors to v, k, q\n",
    "        value, key, query = self.projection(attended, attending)\n",
    "        value, key, query = self.reshape_into_nheads(value, self.nheads, self.d_k), self.reshape_into_nheads(key, self.nheads, self.d_k), self.reshape_into_nheads(query, self.nheads, self.d_k)\n",
    "                \n",
    "        attn_output, attn_weights = scaled_dot_product_attention(value, key, query)\n",
    "        assert attn_output.shape == (attending.shape[-3], self.batch_size * self.nheads, self.d_k), \"attn_output's shape from SDP should be (..., T, N * H, E / H)\"\n",
    "\n",
    "        attn_output = self.reshape_into_nheads(attn_output, 1, d_model)\n",
    "        attn_output = self.W_o(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "    def reshape_into_nheads(self, x, nheads, last_dim):\n",
    "        return x.reshape(-1, self.batch_size * nheads, last_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.nn import InProjContainer, MultiheadAttentionContainer, ScaledDotProduct\n",
    "\n",
    "d_model, num_heads, bsz = 10, 5, 64\n",
    "query = torch.rand((21, bsz, d_model))\n",
    "key = value = torch.rand((16, bsz, d_model))\n",
    "\n",
    "# call torchtext's InProjContainer and then MHA\n",
    "torch.manual_seed(42)\n",
    "l1 = torch.nn.Linear(d_model, d_model)\n",
    "torch.manual_seed(42)\n",
    "l2 = torch.nn.Linear(d_model, d_model)\n",
    "torch.manual_seed(42)\n",
    "l3 = torch.nn.Linear(d_model, d_model)\n",
    "in_proj_container = InProjContainer(l1, l2, l3)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "l4 = torch.nn.Linear(d_model, d_model)\n",
    "MHA = MultiheadAttentionContainer(num_heads, \n",
    "                                  in_proj_container, \n",
    "                                  ScaledDotProduct(),\n",
    "                                  l4)\n",
    "\n",
    "expected_attn_output, expected_attn_weights = MHA(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MHA = MultiheadAttention(num_heads, d_model)\n",
    "attn_output, attn_weights = MY_MHA(key, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert expected_attn_output.shape == attn_output.shape\n",
    "assert expected_attn_weights.shape == expected_attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:        ...OK!\n"
     ]
    }
   ],
   "source": [
    "assertEqual(expected_attn_output, attn_output)\n",
    "assertEqual(expected_attn_outpu, attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0533,  0.0026,  0.1077,  ..., -0.2882, -0.4972, -0.3430],\n",
       "         [ 0.0465,  0.0241,  0.0815,  ..., -0.3206, -0.4755, -0.3576],\n",
       "         [ 0.1169,  0.0276,  0.1214,  ..., -0.3538, -0.5311, -0.3243],\n",
       "         ...,\n",
       "         [ 0.1029,  0.0102,  0.1307,  ..., -0.2922, -0.5349, -0.3298],\n",
       "         [ 0.0462,  0.0226,  0.1029,  ..., -0.3630, -0.4679, -0.3299],\n",
       "         [ 0.0536,  0.0316,  0.0972,  ..., -0.3049, -0.4821, -0.3343]],\n",
       "\n",
       "        [[ 0.0548,  0.0070,  0.0987,  ..., -0.2948, -0.4959, -0.3506],\n",
       "         [ 0.0459,  0.0252,  0.0811,  ..., -0.3238, -0.4733, -0.3561],\n",
       "         [ 0.1115,  0.0290,  0.1223,  ..., -0.3584, -0.5244, -0.3222],\n",
       "         ...,\n",
       "         [ 0.1040,  0.0139,  0.1253,  ..., -0.2947, -0.5349, -0.3327],\n",
       "         [ 0.0484,  0.0221,  0.1069,  ..., -0.3661, -0.4695, -0.3273],\n",
       "         [ 0.0438,  0.0345,  0.0907,  ..., -0.3074, -0.4727, -0.3376]],\n",
       "\n",
       "        [[ 0.0519,  0.0042,  0.1042,  ..., -0.2912, -0.4941, -0.3470],\n",
       "         [ 0.0420,  0.0256,  0.0769,  ..., -0.3214, -0.4698, -0.3595],\n",
       "         [ 0.1127,  0.0290,  0.1222,  ..., -0.3574, -0.5257, -0.3224],\n",
       "         ...,\n",
       "         [ 0.0995,  0.0110,  0.1282,  ..., -0.2943, -0.5316, -0.3315],\n",
       "         [ 0.0441,  0.0234,  0.1039,  ..., -0.3599, -0.4663, -0.3271],\n",
       "         [ 0.0505,  0.0330,  0.0936,  ..., -0.3044, -0.4796, -0.3369]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0531,  0.0026,  0.1065,  ..., -0.2905, -0.4962, -0.3454],\n",
       "         [ 0.0416,  0.0263,  0.0774,  ..., -0.3245, -0.4694, -0.3589],\n",
       "         [ 0.1124,  0.0307,  0.1243,  ..., -0.3613, -0.5239, -0.3187],\n",
       "         ...,\n",
       "         [ 0.1008,  0.0111,  0.1301,  ..., -0.2959, -0.5324, -0.3289],\n",
       "         [ 0.0440,  0.0245,  0.1027,  ..., -0.3634, -0.4653, -0.3285],\n",
       "         [ 0.0508,  0.0333,  0.0951,  ..., -0.3107, -0.4779, -0.3349]],\n",
       "\n",
       "        [[ 0.0502,  0.0035,  0.1025,  ..., -0.2970, -0.4917, -0.3492],\n",
       "         [ 0.0423,  0.0333,  0.0732,  ..., -0.3176, -0.4695, -0.3565],\n",
       "         [ 0.1080,  0.0309,  0.1204,  ..., -0.3593, -0.5200, -0.3221],\n",
       "         ...,\n",
       "         [ 0.1014,  0.0139,  0.1251,  ..., -0.2946, -0.5326, -0.3322],\n",
       "         [ 0.0434,  0.0253,  0.1007,  ..., -0.3615, -0.4649, -0.3295],\n",
       "         [ 0.0502,  0.0297,  0.0988,  ..., -0.3108, -0.4782, -0.3338]],\n",
       "\n",
       "        [[ 0.0605,  0.0037,  0.1069,  ..., -0.2939, -0.5020, -0.3462],\n",
       "         [ 0.0433,  0.0326,  0.0751,  ..., -0.3224, -0.4703, -0.3562],\n",
       "         [ 0.1097,  0.0277,  0.1237,  ..., -0.3599, -0.5222, -0.3212],\n",
       "         ...,\n",
       "         [ 0.0998,  0.0092,  0.1298,  ..., -0.2952, -0.5325, -0.3315],\n",
       "         [ 0.0428,  0.0230,  0.1031,  ..., -0.3590, -0.4653, -0.3283],\n",
       "         [ 0.0507,  0.0352,  0.0920,  ..., -0.3064, -0.4782, -0.3370]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.3185e-02,  2.9902e-02,  1.0467e-01,  ..., -2.8879e-01,\n",
       "          -4.7713e-01, -3.2269e-01],\n",
       "         [ 5.2500e-02,  6.7092e-02,  3.3881e-02,  ..., -2.6797e-01,\n",
       "          -4.6646e-01, -3.6747e-01],\n",
       "         [ 1.4379e-01,  2.8407e-02,  1.1836e-01,  ..., -3.2853e-01,\n",
       "          -5.4493e-01, -3.3934e-01],\n",
       "         ...,\n",
       "         [ 8.1907e-02, -9.6690e-03,  1.2771e-01,  ..., -2.6678e-01,\n",
       "          -5.2813e-01, -3.4869e-01],\n",
       "         [ 3.1156e-02,  1.8001e-02,  1.1451e-01,  ..., -4.2149e-01,\n",
       "          -4.5648e-01, -3.1561e-01],\n",
       "         [ 6.9101e-02,  1.1207e-02,  1.0459e-01,  ..., -2.4799e-01,\n",
       "          -5.1506e-01, -3.4459e-01]],\n",
       "\n",
       "        [[ 4.8109e-02,  1.4327e-02,  8.6957e-02,  ..., -3.0937e-01,\n",
       "          -4.9390e-01, -3.4983e-01],\n",
       "         [ 4.5355e-02,  2.4181e-02,  6.5460e-02,  ..., -3.0616e-01,\n",
       "          -4.7185e-01, -3.8532e-01],\n",
       "         [ 1.3692e-01,  3.3717e-02,  1.3769e-01,  ..., -3.5911e-01,\n",
       "          -5.4853e-01, -3.0835e-01],\n",
       "         ...,\n",
       "         [ 8.1717e-02,  7.3155e-02,  1.0844e-01,  ..., -2.8709e-01,\n",
       "          -4.9356e-01, -2.8396e-01],\n",
       "         [ 2.1906e-02,  6.8086e-03,  1.1863e-01,  ..., -3.9215e-01,\n",
       "          -4.5674e-01, -3.1897e-01],\n",
       "         [ 3.6293e-03,  5.5792e-02,  8.0965e-02,  ..., -3.5314e-01,\n",
       "          -4.3601e-01, -3.0895e-01]],\n",
       "\n",
       "        [[ 4.0564e-02,  6.6629e-03,  1.0488e-01,  ..., -2.8706e-01,\n",
       "          -4.9581e-01, -3.2444e-01],\n",
       "         [ 4.6282e-02,  6.7425e-02,  7.4556e-02,  ..., -3.4969e-01,\n",
       "          -4.6266e-01, -3.1522e-01],\n",
       "         [ 1.2578e-01,  4.0920e-02,  1.3932e-01,  ..., -3.7248e-01,\n",
       "          -5.3377e-01, -2.9290e-01],\n",
       "         ...,\n",
       "         [ 9.3470e-02,  1.4097e-02,  1.2930e-01,  ..., -2.6474e-01,\n",
       "          -5.2756e-01, -3.2794e-01],\n",
       "         [ 1.0168e-02,  1.0056e-02,  1.2645e-01,  ..., -4.1193e-01,\n",
       "          -4.3845e-01, -3.1303e-01],\n",
       "         [ 6.4348e-02,  3.9667e-02,  1.0283e-01,  ..., -3.0177e-01,\n",
       "          -4.9496e-01, -3.2782e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.4784e-02,  7.9837e-04,  9.9706e-02,  ..., -2.6058e-01,\n",
       "          -4.9379e-01, -3.5414e-01],\n",
       "         [ 5.1651e-02, -5.3301e-05,  1.0242e-01,  ..., -3.0612e-01,\n",
       "          -4.8556e-01, -3.6428e-01],\n",
       "         [ 1.1409e-01,  3.7740e-02,  1.2104e-01,  ..., -3.9850e-01,\n",
       "          -5.3226e-01, -3.0735e-01],\n",
       "         ...,\n",
       "         [ 5.7128e-02, -3.6052e-03,  1.4221e-01,  ..., -3.3182e-01,\n",
       "          -4.9818e-01, -3.0593e-01],\n",
       "         [ 4.7544e-02,  3.2352e-02,  1.0277e-01,  ..., -3.7093e-01,\n",
       "          -4.6565e-01, -3.1891e-01],\n",
       "         [ 7.0474e-02,  5.1815e-02,  7.7982e-02,  ..., -2.7098e-01,\n",
       "          -4.9362e-01, -3.3202e-01]],\n",
       "\n",
       "        [[ 5.0932e-02,  2.6641e-02,  9.8917e-02,  ..., -2.8344e-01,\n",
       "          -4.8184e-01, -3.3640e-01],\n",
       "         [ 4.9360e-02,  3.1405e-02,  8.4859e-02,  ..., -3.5033e-01,\n",
       "          -4.7214e-01, -3.4969e-01],\n",
       "         [ 1.0637e-01,  1.0857e-02,  1.4767e-01,  ..., -3.6786e-01,\n",
       "          -5.3002e-01, -3.1020e-01],\n",
       "         ...,\n",
       "         [ 1.0405e-01, -1.9917e-02,  1.3428e-01,  ..., -2.9958e-01,\n",
       "          -5.4355e-01, -3.6327e-01],\n",
       "         [ 1.0541e-02,  2.6562e-02,  1.1737e-01,  ..., -4.2105e-01,\n",
       "          -4.3805e-01, -3.0014e-01],\n",
       "         [ 3.3238e-02, -2.5654e-02,  1.1900e-01,  ..., -3.4597e-01,\n",
       "          -4.7288e-01, -3.6083e-01]],\n",
       "\n",
       "        [[ 7.1186e-02,  1.0038e-03,  1.0552e-01,  ..., -3.1705e-01,\n",
       "          -5.1639e-01, -3.4911e-01],\n",
       "         [ 3.0104e-02,  2.4361e-02,  8.3433e-02,  ..., -3.6258e-01,\n",
       "          -4.5748e-01, -3.5859e-01],\n",
       "         [ 1.0768e-01,  1.3324e-02,  1.3771e-01,  ..., -4.0266e-01,\n",
       "          -5.2358e-01, -3.2639e-01],\n",
       "         ...,\n",
       "         [ 8.2114e-02,  1.8650e-02,  1.4461e-01,  ..., -3.2855e-01,\n",
       "          -5.1031e-01, -3.0647e-01],\n",
       "         [ 5.0880e-02,  1.0526e-02,  1.2136e-01,  ..., -3.6277e-01,\n",
       "          -4.7825e-01, -3.2332e-01],\n",
       "         [ 6.0786e-02,  3.9457e-02,  9.5644e-02,  ..., -2.9680e-01,\n",
       "          -4.7673e-01, -3.4210e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37.4668, grad_fn=<CopyBackwards>)\n",
      "tensor(37.5427, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(expected_attn_output))\n",
    "print(torch.norm(attn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
