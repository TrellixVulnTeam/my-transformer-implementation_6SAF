{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.55.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (51.3.3.post20210118)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=128b53b3e4a6d1c8dec59f503d1ec50741df87748eaabb04ccca54273a6077a3\n",
      "  Stored in directory: /private/var/folders/rt/ppzpkmzd72335rvk3gsxw3q40000gn/T/pip-ephem-wheel-cache-hgom4g2f/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Collecting de_core_news_sm==2.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 625 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from de_core_news_sm==2.3.0) (2.3.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.7.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.9.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.8.1)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (51.3.3.post20210118)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.55.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.19.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.0.0)\n",
      "Building wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.3.0-py3-none-any.whl size=14907581 sha256=aaceca6fcf88586925a4e859e6b351ddcdbb6343d5a91b12be56d47473e5a63e\n",
      "  Stored in directory: /private/var/folders/rt/ppzpkmzd72335rvk3gsxw3q40000gn/T/pip-ephem-wheel-cache-gbo8mv8s/wheels/75/30/c3/ea1c6002eede7f49c8ab017ce62a2981a87b1cd39fab6e6a65\n",
      "Successfully built de-core-news-sm\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-2.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages/de_core_news_sm\n",
      "-->\n",
      "/opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196884/196884 [00:46<00:00, 4209.42lines/s]\n",
      "100%|██████████| 196884/196884 [00:41<00:00, 4726.86lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.experimental.datasets import IWSLT\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# tokenize the dataset (pairs of sentence strings -> pairs of arrays of indices)\n",
    "src_tokenizer = get_tokenizer(\"spacy\", language='de')\n",
    "tgt_tokenizer = get_tokenizer(\"spacy\", language='en')\n",
    "train_dataset, valid_dataset, test_dataset = IWSLT(tokenizer=(src_tokenizer, tgt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchtext\n",
      "Version: 0.9.0.dev20210123\n",
      "Summary: Text utilities and datasets for PyTorch\n",
      "Home-page: https://github.com/pytorch/text\n",
      "Author: PyTorch core devs and James Bradbury\n",
      "Author-email: jekbradbury@gmail.com\n",
      "License: BSD\n",
      "Location: /opt/anaconda3/envs/test-transformer/lib/python3.7/site-packages\n",
      "Requires: tqdm, requests, torch, numpy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!python -m pip show torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.', '\\n'], ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.', '\\n'])\n",
      "(['Wir', 'werden', 'Ihnen', 'einige', 'Geschichten', 'über', 'das', 'Meer', 'in', 'Videoform', 'erzählen', '.', '\\n'], ['And', 'we', \"'re\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.', '\\n'])\n",
      "(['Wir', 'haben', 'ein', 'paar', 'der', 'unglaublichsten', 'Aufnahmen', 'der', 'Titanic', ',', 'die', 'man', 'je', 'gesehen', 'hat', ',', ',', 'und', 'wir', 'werden', 'Ihnen', 'nichts', 'davon', 'zeigen', '.', '\\n'], ['We', \"'ve\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', 'that', \"'s\", 'ever', 'been', 'seen', ',', 'and', 'we', \"'re\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.', '\\n'])\n",
      "(['Die', 'Wahrheit', 'ist', ',', 'dass', 'die', 'Titanic', '–', 'obwohl', 'sie', 'alle', 'Kinokassenrekorde', 'bricht', '–', 'nicht', 'gerade', 'die', 'aufregendste', 'Geschichte', 'vom', 'Meer', 'ist', '.', '\\n'], ['The', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', 'it', \"'s\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', 'it', \"'s\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.', '\\n'])\n",
      "(['Ich', 'denke', ',', 'das', 'Problem', 'ist', ',', 'dass', 'wir', 'das', 'Meer', 'für', 'zu', 'selbstverständlich', 'halten', '.', '\\n'], ['And', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.', '\\n'])\n",
      "(['Wenn', 'man', 'darüber', 'nachdenkt', ',', 'machen', 'die', 'Ozeane', '75', '%', 'des', 'Planeten', 'aus', '.', '\\n'], ['When', 'you', 'think', 'about', 'it', ',', 'the', 'oceans', 'are', '75', 'percent', 'of', 'the', 'planet', '.', '\\n'])\n",
      "(['Der', 'Großteil', 'der', 'Erde', 'ist', 'Meerwasser', '.', '\\n'], ['Most', 'of', 'the', 'planet', 'is', 'ocean', 'water', '.', '\\n'])\n",
      "(['Die', 'durchschnittliche', 'Tiefe', 'ist', 'etwa', '3', 'Kilometer', '.', '\\n'], ['The', 'average', 'depth', 'is', 'about', 'two', 'miles', '.', '\\n'])\n",
      "(['Ein', 'Teil', 'des', 'Problems', 'ist', ',', 'dass', 'wir', 'am', 'Strand', 'stehen', 'oder', 'Bilder', 'wie', 'dieses', 'hier', 'sehen', 'und', 'auf', 'die', 'riesige', 'blaue', 'Weite', 'schauen', ',', 'und', 'sie', 'schimmert', 'und', 'bewegt', 'sich', ',', 'es', 'gibt', 'Wellen', ',', 'Brandung', 'und', 'Gezeiten', ',', 'aber', 'wir', 'haben', 'keine', 'Ahnung', ',', 'was', 'darin', 'verborgen', 'ist', '.', '\\n'], ['Part', 'of', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'we', 'stand', 'at', 'the', 'beach', ',', 'or', 'we', 'see', 'images', 'like', 'this', 'of', 'the', 'ocean', ',', 'and', 'you', 'look', 'out', 'at', 'this', 'great', 'big', 'blue', 'expanse', ',', 'and', 'it', \"'s\", 'shimmering', 'and', 'it', \"'s\", 'moving', 'and', 'there', \"'s\", 'waves', 'and', 'there', \"'s\", 'surf', 'and', 'there', \"'s\", 'tides', ',', 'but', 'you', 'have', 'no', 'idea', 'for', 'what', 'lies', 'in', 'there', '.', '\\n'])\n",
      "(['In', 'den', 'Ozeanen', 'befinden', 'sich', 'die', 'längsten', 'Gebirgszüge', 'des', 'Planeten', '.', '\\n'], ['And', 'in', 'the', 'oceans', ',', 'there', 'are', 'the', 'longest', 'mountain', 'ranges', 'on', 'the', 'planet', '.', '\\n'])\n",
      "(['Die', 'meisten', 'Tiere', 'leben', 'in', 'den', 'Ozeanen', '.', '\\n'], ['Most', 'of', 'the', 'animals', 'are', 'in', 'the', 'oceans', '.', '\\n'])\n",
      "(['Der', 'Großteil', 'der', 'Erdbeben', 'und', 'Vulkanausbrüche', 'spielt', 'sich', 'im', 'Meer', 'ab', '–', 'am', 'Meeresboden', '.', '\\n'], ['Most', 'of', 'the', 'earthquakes', 'and', 'volcanoes', 'are', 'in', 'the', 'sea', ',', 'at', 'the', 'bottom', 'of', 'the', 'sea', '.', '\\n'])\n",
      "(['An', 'einigen', 'Stellen', 'ist', 'die', 'Vielfalt', 'und', 'die', 'Dichte', 'des', 'Lebens', 'im', 'Ozean', 'höher', 'als', 'in', 'den', 'Regenwäldern', '.', '\\n'], ['The', 'biodiversity', 'and', 'the', 'biodensity', 'in', 'the', 'ocean', 'is', 'higher', ',', 'in', 'places', ',', 'than', 'it', 'is', 'in', 'the', 'rainforests', '.', '\\n'])\n",
      "(['Das', 'meiste', 'ist', 'unerforscht', ',', 'und', 'doch', 'gibt', 'es', 'Schönheiten', 'wie', 'diese', ',', 'die', 'uns', 'fesseln', 'und', 'uns', 'vertrauter', 'mit', 'ihm', 'machen', '.', '\\n'], ['It', \"'s\", 'mostly', 'unexplored', ',', 'and', 'yet', 'there', 'are', 'beautiful', 'sights', 'like', 'this', 'that', 'captivate', 'us', 'and', 'make', 'us', 'become', 'familiar', 'with', 'it', '.', '\\n'])\n",
      "(['Aber', 'wenn', 'Sie', 'am', 'Strand', 'stehen', ',', 'möchte', 'ich', ',', 'dass', 'Sie', 'sich', 'vorstellen', ',', 'an', 'der', 'Grenze', 'zu', 'einer', 'unbekannten', 'Welt', 'zu', 'stehen', '.', '\\n'], ['But', 'when', 'you', \"'re\", 'standing', 'at', 'the', 'beach', ',', 'I', 'want', 'you', 'to', 'think', 'that', 'you', \"'re\", 'standing', 'at', 'the', 'edge', 'of', 'a', 'very', 'unfamiliar', 'world', '.', '\\n'])\n",
      "(['Wir', 'müssen', 'uns', 'schon', 'einer', 'sehr', 'speziellen', 'Technik', 'bedienen', ',', 'um', 'in', 'diese', 'unbekannte', 'Welt', 'vorzudringen', '.', '\\n'], ['We', 'have', 'to', 'have', 'a', 'very', 'special', 'technology', 'to', 'get', 'into', 'that', 'unfamiliar', 'world', '.', '\\n'])\n",
      "(['Wir', 'benutzen', 'das', 'Unterseeboot', 'Alvin', 'und', 'Kameras', ',', 'und', 'diese', 'Kameras', 'hat', 'Bill', 'Lange', 'mit', 'Hilfe', 'von', 'Sony', 'entwickelt', '.', '\\n'], ['We', 'use', 'the', 'submarine', 'Alvin', 'and', 'we', 'use', 'cameras', ',', 'and', 'the', 'cameras', 'are', 'something', 'that', 'Bill', 'Lange', 'has', 'developed', 'with', 'the', 'help', 'of', 'Sony', '.', '\\n'])\n",
      "(['Marcel', 'Proust', 'hat', 'einmal', 'gesagt', ':', '\"', 'Die', 'wahre', 'Entdeckungsreise', 'besteht', 'nicht', 'darin', ',', 'dass', 'man', 'neue', 'Länder', 'sucht', ',', 'sondern', 'dass', 'man', 'neue', 'Augen', 'hat', '.', '\"', '\\n'], ['Marcel', 'Proust', 'said', ',', '\"', 'The', 'true', 'voyage', 'of', 'discovery', 'is', 'not', 'so', 'much', 'in', 'seeking', 'new', 'landscapes', 'as', 'in', 'having', 'new', 'eyes', '.', '\"', '\\n'])\n",
      "(['Die', 'Menschen', ',', 'die', 'mit', 'uns', 'zusammengearbeitet', 'haben', ',', 'gaben', 'uns', 'neue', 'Augen', ',', 'nicht', 'nur', ',', 'um', 'das', 'zu', 'sehen', ',', 'was', 'existiert', '–', 'die', 'neuen', 'Länder', 'auf', 'dem', 'Meeresboden', '–', 'sondern', 'auch', ',', 'um', 'über', 'das', 'Leben', 'auf', 'diesem', 'Planeten', 'nachzudenken', '.', '\\n'], ['People', 'that', 'have', 'partnered', 'with', 'us', 'have', 'given', 'us', 'new', 'eyes', ',', 'not', 'only', 'on', 'what', 'exists', '--', 'the', 'new', 'landscapes', 'at', 'the', 'bottom', 'of', 'the', 'sea', '--', 'but', 'also', 'how', 'we', 'think', 'about', 'life', 'on', 'the', 'planet', 'itself', '.', '\\n'])\n",
      "(['Das', 'ist', 'ein', 'Weichtier', '.', '\\n'], ['Here', \"'s\", 'a', 'jelly', '.', '\\n'])\n",
      "(['Es', 'ist', 'einer', 'meiner', 'Lieblinge', ',', 'weil', 'es', 'alle', 'möglichen', 'Funktionsteile', 'hat', '.', '\\n'], ['It', \"'s\", 'one', 'of', 'my', 'favorites', ',', 'because', 'it', \"'s\", 'got', 'all', 'sorts', 'of', 'working', 'parts', '.', '\\n'])\n",
      "(['Es', 'hat', 'sich', 'als', 'das', 'längste', 'Wesen', 'im', 'Meer', 'erwiesen', '.', '\\n'], ['This', 'turns', 'out', 'to', 'be', 'the', 'longest', 'creature', 'in', 'the', 'oceans', '.', '\\n'])\n",
      "(['Es', 'wird', 'bis', 'zu', '50', 'Meter', 'lang', '.', '\\n'], ['It', 'gets', 'up', 'to', 'about', '150', 'feet', 'long', '.', '\\n'])\n",
      "(['Sehen', 'Sie', 'all', 'die', 'unterschiedlichen', 'Teile', '?', '\\n'], ['But', 'see', 'all', 'those', 'different', 'working', 'things', '?', '\\n'])\n",
      "(['So', 'was', 'mag', 'ich', '.', '\\n'], ['I', 'love', 'that', 'kind', 'of', 'stuff', '.', '\\n'])\n",
      "(['Es', 'hat', 'diese', 'Fischköder', 'an', 'der', 'Unterseite', '.', 'Die', 'bewegen', 'sich', 'auf', 'und', 'ab', '.', '\\n'], ['It', \"'s\", 'got', 'these', 'fishing', 'lures', 'on', 'the', 'bottom', '.', 'They', \"'re\", 'going', 'up', 'and', 'down', '.', '\\n'])\n",
      "(['Es', 'hat', 'Tentakel', ',', 'die', 'so', 'herumschwirren', '.', '\\n'], ['It', \"'s\", 'got', 'tentacles', 'dangling', ',', 'swirling', 'around', 'like', 'that', '.', '\\n'])\n",
      "(['Es', 'ist', 'eine', 'Kolonie', 'von', 'Tieren', '.', '\\n'], ['It', \"'s\", 'a', 'colonial', 'animal', '.', '\\n'])\n",
      "(['Das', 'sind', 'alles', 'einzelne', 'Lebewesen', ',', 'die', 'sich', 'zu', 'diesem', 'einen', 'Organismus', 'zusammenschließen', '.', '\\n'], ['These', 'are', 'all', 'individual', 'animals', 'banding', 'together', 'to', 'make', 'this', 'one', 'creature', '.', '\\n'])\n",
      "(['Es', 'hat', 'vorne', 'diese', 'Antriebsdüsen', ',', 'die', 'es', 'gleich', 'benutzt', ',', 'und', 'ein', 'kleines', 'Licht', '.', '\\n'], ['And', 'it', \"'s\", 'got', 'these', 'jet', 'thrusters', 'up', 'in', 'front', 'that', 'it', \"'ll\", 'use', 'in', 'a', 'moment', ',', 'and', 'a', 'little', 'light', '.', '\\n'])\n",
      "(['Wenn', 'man', 'alle', 'großen', 'Fische', 'und', 'Fischschulen', 'und', 'all', 'das', 'nimmt', 'und', 'auf', 'die', 'eine', 'Seite', 'der', 'Waage', 'legt', 'und', 'alle', 'Weichtiere', 'auf', 'die', 'andere', 'Seite', ',', 'gewinnen', 'die', 'hier', 'haushoch', '.', '\\n'], ['If', 'you', 'take', 'all', 'the', 'big', 'fish', 'and', 'schooling', 'fish', 'and', 'all', 'that', ',', 'put', 'them', 'on', 'one', 'side', 'of', 'the', 'scale', ',', 'put', 'all', 'the', 'jelly', '-', 'type', 'of', 'animals', 'on', 'the', 'other', 'side', ',', 'those', 'guys', 'win', 'hands', 'down', '.', '\\n'])\n",
      "(['Der', 'Großteil', 'der', 'Biomasse', 'im', 'Meer', 'besteht', 'aus', 'Geschöpfen', 'wie', 'diesem', '.', '\\n'], ['Most', 'of', 'the', 'biomass', 'in', 'the', 'ocean', 'is', 'made', 'out', 'of', 'creatures', 'like', 'this', '.', '\\n'])\n",
      "(['Hier', 'ist', 'das', 'X-Wing-Todes-Weichtier', '.', '\\n'], ['Here', \"'s\", 'the', 'X', '-', 'wing', 'death', 'jelly', '.', '\\n'])\n",
      "(['Sie', 'benutzen', 'die', 'Biolumineszenz', ',', 'um', 'Geschlechtspartner', 'anzulocken', ',', 'Beute', 'zu', 'ködern', 'und', 'zur', 'Verständigung', '.', '\\n'], ['The', 'bioluminescence', '--', 'they', 'use', 'the', 'lights', 'for', 'attracting', 'mates', 'and', 'attracting', 'prey', 'and', 'communicating', '.', '\\n'])\n",
      "(['Wir', 'hätten', 'niemals', 'die', 'Zeit', ',', 'Ihnen', 'all', 'unser', 'Archivmaterial', 'von', 'den', 'Weichtieren', 'zu', 'zeigen', '.', '\\n'], ['We', 'could', \"n't\", 'begin', 'to', 'show', 'you', 'our', 'archival', 'stuff', 'from', 'the', 'jellies', '.', '\\n'])\n",
      "(['Es', 'gibt', 'sie', 'in', 'den', 'unterschiedlichsten', 'Größen', 'und', 'Formen', '.', '\\n'], ['They', 'come', 'in', 'all', 'different', 'sizes', 'and', 'shapes', '.', '\\n'])\n",
      "(['Bill', 'Lange', ':', 'Wir', 'vergessen', 'leicht', ',', 'dass', 'das', 'Meer', 'durchschnittlich', 'mehrere', 'Kilometer', 'tief', 'ist', 'und', 'dass', 'wir', 'nur', 'die', 'Tiere', 'wirklich', 'kennen', ',', 'die', 'in', 'den', 'oberen', 'hundert', 'Metern', 'leben', ',', 'aber', 'was', 'zwischen', 'dort', 'und', 'dem', 'Meeresboden', 'lebt', ',', 'ist', 'uns', 'nicht', 'vertraut', '.', '\\n'], ['Bill', 'Lange', ':', 'We', 'tend', 'to', 'forget', 'about', 'the', 'fact', 'that', 'the', 'ocean', 'is', 'miles', 'deep', 'on', 'average', ',', 'and', 'that', 'we', \"'re\", 'real', 'familiar', 'with', 'the', 'animals', 'that', 'are', 'in', 'the', 'first', '200', 'or', '300', 'feet', ',', 'but', 'we', \"'re\", 'not', 'familiar', 'with', 'what', 'exists', 'from', 'there', 'all', 'the', 'way', 'down', 'to', 'the', 'bottom', '.', '\\n'])\n",
      "(['Diese', 'Tierarten', 'leben', 'in', 'einem', 'dreidimensionalen', 'Raum', ',', 'einem', 'Mikrogravitationsraum', ',', 'den', 'wir', 'überhaupt', 'noch', 'nicht', 'erforscht', 'haben', '.', '\\n'], ['And', 'these', 'are', 'the', 'types', 'of', 'animals', 'that', 'live', 'in', 'that', 'three', '-', 'dimensional', 'space', ',', 'that', 'micro', '-', 'gravity', 'environment', 'that', 'we', 'really', 'have', \"n't\", 'explored', '.', '\\n'])\n",
      "(['Man', 'hört', 'von', 'Riesenkalmaren', 'und', 'so', 'etwas', ',', 'aber', 'einige', 'dieser', 'Tiere', 'werden', 'bis', 'zu', 'etwa', '50', 'Meter', 'lang', '.', '\\n'], ['You', 'hear', 'about', 'giant', 'squid', 'and', 'things', 'like', 'that', ',', 'but', 'some', 'of', 'these', 'animals', 'get', 'up', 'to', 'be', 'approximately', '140', ',', '160', 'feet', 'long', '.', '\\n'])\n",
      "(['Man', 'weiß', 'sehr', 'wenig', 'über', 'sie', '.', '\\n'], ['They', \"'re\", 'very', 'little', 'understood', '.', '\\n'])\n",
      "(['DG', ':', 'Das', 'ist', 'eines', 'von', 'ihnen', ',', 'noch', 'einer', 'unserer', 'Lieblinge', ',', 'denn', 'es', 'ist', 'ein', 'kleiner', 'Oktopus', '.', '\\n'], ['DG', ':', 'This', 'is', 'one', 'of', 'them', ',', 'another', 'one', 'of', 'our', 'favorites', ',', 'because', 'it', \"'s\", 'a', 'little', 'octopod', '.', '\\n'])\n",
      "(['Man', 'kann', 'ihm', 'tatsächlich', 'durch', 'den', 'Kopf', 'gucken', '.', '\\n'], ['You', 'can', 'actually', 'see', 'through', 'his', 'head', '.', '\\n'])\n",
      "(['Und', 'hier', 'wackelt', 'er', 'mit', 'den', 'Ohren', 'und', 'steigt', 'sehr', 'anmutig', 'nach', 'oben', '.', '\\n'], ['And', 'here', 'he', 'is', ',', 'flapping', 'with', 'his', 'ears', 'and', 'very', 'gracefully', 'going', 'up', '.', '\\n'])\n",
      "(['Wir', 'finden', 'sie', 'in', 'allen', 'Tiefen', 'und', 'sogar', 'in', 'die', 'tiefsten', 'Tiefen', '.', '\\n'], ['We', 'see', 'those', 'at', 'all', 'depths', 'and', 'even', 'at', 'the', 'greatest', 'depths', '.', '\\n'])\n",
      "(['Sie', 'sind', 'von', 'wenigen', 'Zentimetern', 'bis', 'hin', 'zu', 'ein', 'paar', 'Metern', 'lang', '.', '\\n'], ['They', 'go', 'from', 'a', 'couple', 'of', 'inches', 'to', 'a', 'couple', 'of', 'feet', '.', '\\n'])\n",
      "(['Sie', 'kommen', 'bis', 'ans', 'U-Boot', 'heran', '–', 'sie', 'kommen', 'mit', 'den', 'Augen', 'ans', 'Fenster', 'und', 'gucken', 'ins', 'U-Boot', '.', '\\n'], ['They', 'come', 'right', 'up', 'to', 'the', 'submarine', '--', 'they', \"'ll\", 'put', 'their', 'eyes', 'right', 'up', 'to', 'the', 'window', 'and', 'peek', 'inside', 'the', 'sub', '.', '\\n'])\n",
      "(['Es', 'ist', 'eine', 'eigene', 'Welt', 'in', 'der', 'Welt', ',', 'und', 'wir', 'werden', 'Ihnen', 'zwei', 'zeigen', '.', '\\n'], ['This', 'is', 'really', 'a', 'world', 'within', 'a', 'world', ',', 'and', 'we', \"'re\", 'going', 'to', 'show', 'you', 'two', '.', '\\n'])\n",
      "(['In', 'diesem', 'Fall', 'bewegen', 'wir', 'uns', 'durch', 'die', 'Ozeanmitte', 'nach', 'unten', 'und', 'sehen', 'solche', 'Kreaturen', '.', '\\n'], ['In', 'this', 'case', ',', 'we', \"'re\", 'passing', 'down', 'through', 'the', 'mid', '-', 'ocean', 'and', 'we', 'see', 'creatures', 'like', 'this', '.', '\\n'])\n",
      "(['Das', 'hier', 'ist', 'eine', 'Art', 'Unterwasser-Hahn', '.', '\\n'], ['This', 'is', 'kind', 'of', 'like', 'an', 'undersea', 'rooster', '.', '\\n'])\n",
      "(['Der', 'hier', ',', 'der', 'irgendwie', 'unglaublich', 'förmlich', 'aussieht', '.', '\\n'], ['This', 'guy', ',', 'that', 'looks', 'incredibly', 'formal', ',', 'in', 'a', 'way', '.', '\\n'])\n",
      "(['Und', 'hier', 'ist', 'einer', 'meiner', 'Lieblinge', '.', 'Was', 'für', 'ein', 'Gesicht', '!', '\\n'], ['And', 'then', 'one', 'of', 'my', 'favorites', '.', 'What', 'a', 'face', '!', '\\n'])\n",
      "(['Was', 'sie', 'hier', 'sehen', ',', 'sind', 'im', 'Grunde', 'wissenschaftliche', 'Daten', '.', '\\n'], ['This', 'is', 'basically', 'scientific', 'data', 'that', 'you', \"'re\", 'looking', 'at', '.', '\\n'])\n",
      "(['Es', 'sind', 'Aufnahmen', ',', 'die', 'wir', 'zu', 'Forschungszwecken', 'gemacht', 'haben', '.', '\\n'], ['It', \"'s\", 'footage', 'that', 'we', \"'ve\", 'collected', 'for', 'scientific', 'purposes', '.', '\\n'])\n",
      "(['Das', 'ist', 'auch', 'etwas', ',', 'das', 'Bill', 'macht', ',', 'Wissenschaftler', 'mit', 'den', 'ersten', 'Bildern', 'von', 'diesen', 'Tieren', 'zu', 'versorgen', ',', 'in', 'ihrer', 'natürlichen', 'Umgebung', '.', '\\n'], ['And', 'that', \"'s\", 'one', 'of', 'the', 'things', 'that', 'Bill', \"'s\", 'been', 'doing', ',', 'is', 'providing', 'scientists', 'with', 'this', 'first', 'view', 'of', 'animals', 'like', 'this', ',', 'in', 'the', 'world', 'where', 'they', 'belong', '.', '\\n'])\n",
      "(['Sie', 'fangen', 'sie', 'nicht', 'mit', 'einem', 'Netz', '.', '\\n'], ['They', 'do', \"n't\", 'catch', 'them', 'in', 'a', 'net', '.', '\\n'])\n",
      "(['Sie', 'sehen', 'sie', 'sich', 'da', 'unten', 'in', 'dieser', 'Welt', 'an', '.', '\\n'], ['They', \"'re\", 'actually', 'looking', 'at', 'them', 'down', 'in', 'that', 'world', '.', '\\n'])\n",
      "(['Wir', 'nehmen', 'einen', 'Joystick', ',', 'sitzen', 'vor', 'unserem', 'Computer', 'an', 'Land', ',', 'drücken', 'den', 'Joystick', 'nach', 'vorn', 'und', 'fliegen', 'um', 'den', 'Planeten', '.', '\\n'], ['We', \"'re\", 'going', 'to', 'take', 'a', 'joystick', ',', 'sit', 'in', 'front', 'of', 'our', 'computer', ',', 'on', 'the', 'Earth', ',', 'and', 'press', 'the', 'joystick', 'forward', ',', 'and', 'fly', 'around', 'the', 'planet', '.', '\\n'])\n",
      "(['Wir', 'werden', 'uns', 'den', 'mittelozeanischen', 'Rücken', 'ansehen', ',', 'ein', '64.000', 'Kilometer', 'langer', 'Gebirgszug', '.', '\\n'], ['We', \"'re\", 'going', 'to', 'look', 'at', 'the', 'mid', '-', 'ocean', 'ridge', ',', 'a', '40,000-mile', 'long', 'mountain', 'range', '.', '\\n'])\n",
      "(['Die', 'durchschnittliche', 'Tiefe', 'der', 'Gipfel', 'ist', 'etwa', '2,5', 'Kilometer', '.', '\\n'], ['The', 'average', 'depth', 'at', 'the', 'top', 'of', 'it', 'is', 'about', 'a', 'mile', 'and', 'a', 'half', '.', '\\n'])\n",
      "(['Jetzt', 'sind', 'wir', 'über', 'dem', 'Atlantik', '–', 'das', 'da', 'ist', 'der', 'Rücken', '–', ',', 'werden', 'die', 'Karibik', ',', 'in', 'Mittelamerika', 'überqueren', 'und', 'im', 'Pazifik', 'ankommen', ',', 'neun', 'Grad', 'nördlicher', 'Breite', '.', '\\n'], ['And', 'we', \"'re\", 'over', 'the', 'Atlantic', '--', 'that', \"'s\", 'the', 'ridge', 'right', 'there', '--', 'but', 'we', \"'re\", 'going', 'to', 'go', 'across', 'the', 'Caribbean', ',', 'Central', 'America', ',', 'and', 'end', 'up', 'against', 'the', 'Pacific', ',', 'nine', 'degrees', 'north', '.', '\\n'])\n",
      "(['Wir', 'erstellen', 'Karten', 'dieser', 'Gebirgszüge', 'mit', 'Hilfe', 'von', 'Schall', ',', 'Sonar', ',', 'und', 'das', 'ist', 'einer', 'dieser', 'Gebirgszüge', '.', '\\n'], ['We', 'make', 'maps', 'of', 'these', 'mountain', 'ranges', 'with', 'sound', ',', 'with', 'sonar', ',', 'and', 'this', 'is', 'one', 'of', 'those', 'mountain', 'ranges', '.', '\\n'])\n",
      "(['Wir', 'biegen', 'um', 'eine', 'Klippe', 'hier', 'rechts', '.', '\\n'], ['We', \"'re\", 'coming', 'around', 'a', 'cliff', 'here', 'on', 'the', 'right', '.', '\\n'])\n",
      "(['Die', 'Höhe', 'dieser', 'Berge', 'auf', 'beiden', 'Seiten', 'des', 'Tals', 'ist', 'größer', 'als', 'die', 'der', 'Alpen', 'in', 'den', 'meisten', 'Fällen', '.', '\\n'], ['The', 'height', 'of', 'these', 'mountains', 'on', 'either', 'side', 'of', 'this', 'valley', 'is', 'greater', 'than', 'the', 'Alps', 'in', 'most', 'cases', '.', '\\n'])\n",
      "(['Und', 'es', 'gibt', 'zehntausende', 'von', 'Bergen', 'hier', 'draußen', ',', 'die', 'auf', 'keiner', 'Karte', 'verzeichnet', 'sind', '.', '\\n'], ['And', 'there', \"'s\", 'tens', 'of', 'thousands', 'of', 'those', 'mountains', 'out', 'there', 'that', 'have', \"n't\", 'been', 'mapped', 'yet', '.', '\\n'])\n",
      "(['Das', 'ist', 'ein', 'vulkanischer', 'Rücken', '.', '\\n'], ['This', 'is', 'a', 'volcanic', 'ridge', '.', '\\n'])\n",
      "(['Wir', 'tauchen', 'immer', 'tiefer', 'und', 'tiefer', '.', '\\n'], ['We', \"'re\", 'getting', 'down', 'further', 'and', 'further', 'in', 'scale', '.', '\\n'])\n",
      "(['Und', 'schließlich', 'kommen', 'wir', 'auf', 'so', 'etwas', 'wie', 'das', 'hier', '.', '\\n'], ['And', 'eventually', ',', 'we', 'can', 'come', 'up', 'with', 'something', 'like', 'this', '.', '\\n'])\n",
      "(['Das', 'ist', 'ein', 'Bild', 'unseres', 'Roboters', '–', 'er', 'heißt', 'Jason', '.', '\\n'], ['This', 'is', 'an', 'icon', 'of', 'our', 'robot', ',', 'Jason', ',', 'it', \"'s\", 'called', '.', '\\n'])\n",
      "(['Man', 'kann', 'in', 'einem', 'Raum', 'wie', 'diesem', 'sitzen', ',', 'mit', 'einem', 'Joystick', 'und', 'einem', 'Headset', 'und', 'so', 'einen', 'Roboter', 'ohne', 'Zeitverzögerung', 'auf', 'dem', 'Meeresboden', 'herumfahren', 'lassen', '.', '\\n'], ['And', 'you', 'can', 'sit', 'in', 'a', 'room', 'like', 'this', ',', 'with', 'a', 'joystick', 'and', 'a', 'headset', ',', 'and', 'drive', 'a', 'robot', 'like', 'that', 'around', 'the', 'bottom', 'of', 'the', 'ocean', 'in', 'real', 'time', '.', '\\n'])\n",
      "(['Unter', 'anderem', 'versuchen', 'wir', 'mit', 'unseren', 'Partnern', 'bei', 'Woods', 'Hole', ',', 'diese', 'virtuelle', 'Welt', '–', 'diese', 'Welt', ',', 'dieses', 'unerforschte', 'Gebiet', '–', 'ins', 'Labor', 'zu', 'holen', '.', '\\n'], ['One', 'of', 'the', 'things', 'we', \"'re\", 'trying', 'to', 'do', 'at', 'Woods', 'Hole', 'with', 'our', 'partners', 'is', 'to', 'bring', 'this', 'virtual', 'world', '--', 'this', 'world', ',', 'this', 'unexplored', 'region', '--', 'back', 'to', 'the', 'laboratory', '.', '\\n'])\n",
      "(['Denn', 'wir', 'sehen', 'sie', 'bisher', 'nur', 'stückweise', '.', '\\n'], ['Because', 'we', 'see', 'it', 'in', 'bits', 'and', 'pieces', 'right', 'now', '.', '\\n'])\n",
      "(['Wir', 'nehmen', 'sie', 'entweder', 'als', 'Geräusch', 'war', 'oder', 'als', 'Video', 'oder', 'auf', 'Fotos', 'oder', 'mit', 'Hilfe', 'von', 'chemischen', 'Sensoren', '–', 'doch', 'wir', 'haben', 'die', 'Teile', 'bisher', 'nicht', 'zu', 'einem', 'interessanten', 'Bild', 'zusammengesetzt', '.', '\\n'], ['We', 'see', 'it', 'either', 'as', 'sound', ',', 'or', 'we', 'see', 'it', 'as', 'video', ',', 'or', 'we', 'see', 'it', 'as', 'photographs', ',', 'or', 'we', 'see', 'it', 'as', 'chemical', 'sensors', ',', 'but', 'we', 'never', 'have', 'yet', 'put', 'it', 'all', 'together', 'into', 'one', 'interesting', 'picture', '.', '\\n'])\n",
      "(['Hier', 'brillieren', 'Bills', 'Kameras', 'richtig', '.', '\\n'], ['Here', \"'s\", 'where', 'Bill', \"'s\", 'cameras', 'really', 'do', 'shine', '.', '\\n'])\n",
      "(['Das', 'hier', 'nennt', 'man', 'eine', 'hydrothermale', 'Quelle', '.', '\\n'], ['This', 'is', 'what', \"'s\", 'called', 'a', 'hydrothermal', 'vent', '.', '\\n'])\n",
      "(['Und', 'hier', 'sieht', 'man', 'eine', 'Wolke', 'von', 'dichtem', 'hydrogensulfidreichem', 'Wasser', ',', 'das', 'aus', 'einer', 'vulkanischen', 'Längsachse', 'am', 'Meeresboden', 'tritt', '.', '\\n'], ['And', 'what', 'you', \"'re\", 'seeing', 'here', 'is', 'a', 'cloud', 'of', 'densely', 'packed', ',', 'hydrogen', '-', 'sulfide', '-', 'rich', 'water', 'coming', 'out', 'of', 'a', 'volcanic', 'axis', 'on', 'the', 'sea', 'floor', '.', '\\n'])\n",
      "(['Es', 'wird', 'bis', 'zu', 'etwa', '300', 'Grad', 'Celsius', 'heiß', '.', '\\n'], ['Gets', 'up', 'to', '600', ',', '700', 'degrees', 'F', ',', 'somewhere', 'in', 'that', 'range', '.', '\\n'])\n",
      "(['Das', 'ist', 'also', 'alles', 'Wasser', 'unter', 'dem', 'Meer', '–', 'zwei', ',', 'drei', ',', 'vier', 'Kilometer', 'in', 'der', 'Tiefe', '.', '\\n'], ['So', 'that', \"'s\", 'all', 'water', 'under', 'the', 'sea', '--', 'a', 'mile', 'and', 'a', 'half', ',', 'two', 'miles', ',', 'three', 'miles', 'down', '.', '\\n'])\n",
      "(['Wir', 'wussten', 'bereits', 'in', 'den', '60ern', 'und', '70ern', ',', 'dass', 'es', 'vulkanisch', 'ist', '.', '\\n'], ['And', 'we', 'knew', 'it', 'was', 'volcanic', 'back', 'in', 'the', \"'\", '60s', ',', \"'\", '70s', '.', '\\n'])\n",
      "(['Damit', 'hatten', 'wir', 'einen', 'Hinweis', ',', 'dass', 'es', 'diese', 'Quellen', 'entlang', 'dieser', 'Achse', 'gibt', ',', 'denn', 'wenn', 'es', 'Vulkanismus', 'gibt', ',', 'dringt', 'Wasser', 'in', 'die', 'Spalten', 'am', 'Meeresboden', ',', 'trifft', 'auf', 'Magma', 'und', 'tritt', 'siedend', 'heiß', 'wieder', 'aus', '.', '\\n'], ['And', 'then', 'we', 'had', 'some', 'hint', 'that', 'these', 'things', 'existed', 'all', 'along', 'the', 'axis', 'of', 'it', ',', 'because', 'if', 'you', \"'ve\", 'got', 'volcanism', ',', 'water', \"'s\", 'going', 'to', 'get', 'down', 'from', 'the', 'sea', 'into', 'cracks', 'in', 'the', 'sea', 'floor', ',', 'come', 'in', 'contact', 'with', 'magma', ',', 'and', 'come', 'shooting', 'out', 'hot', '.', '\\n'])\n",
      "(['Uns', 'war', 'nicht', 'bewusst', ',', 'dass', 'es', 'so', 'reich', 'an', 'Hydrogensulfiden', 'ist', '.', '\\n'], ['We', 'were', \"n't\", 'really', 'aware', 'that', 'it', 'would', 'be', 'so', 'rich', 'with', 'sulfides', ',', 'hydrogen', 'sulfides', '.', '\\n'])\n",
      "(['Wir', 'hatten', 'keine', 'Ahnung', 'von', 'diesen', 'Dingern', ',', 'die', 'wir', 'Schornsteine', 'nennen', '.', '\\n'], ['We', 'did', \"n't\", 'have', 'any', 'idea', 'about', 'these', 'things', ',', 'which', 'we', 'call', 'chimneys', '.', '\\n'])\n",
      "(['Das', 'ist', 'eine', 'dieser', 'hydrothermalen', 'Quellen', '.', '\\n'], ['This', 'is', 'one', 'of', 'these', 'hydrothermal', 'vents', '.', '\\n'])\n",
      "(['300', 'Grad', 'heißes', 'Wasser', 'tritt', 'aus', 'der', 'Erde', '.', '\\n'], ['Six', 'hundred', 'degree', 'F', 'water', 'coming', 'out', 'of', 'the', 'Earth', '.', '\\n'])\n",
      "(['Auf', 'beiden', 'Seiten', 'liegen', 'Gebirgszüge', ',', 'die', 'höher', 'als', 'die', 'Alpen', 'sind', ',', 'diese', 'Gegend', 'hier', 'ist', 'also', 'sehr', 'dramatisch', '.', '\\n'], ['On', 'either', 'side', 'of', 'us', 'are', 'mountain', 'ranges', 'that', 'are', 'higher', 'than', 'the', 'Alps', ',', 'so', 'the', 'setting', 'here', 'is', 'very', 'dramatic', '.', '\\n'])\n",
      "(['BL', ':', 'Das', 'Weiße', 'ist', 'eine', 'Bakterienart', ',', 'die', 'bei', '180', 'Grad', 'Celsius', 'gedeiht', '.', '\\n'], ['BL', ':', 'The', 'white', 'material', 'is', 'a', 'type', 'of', 'bacteria', 'that', 'thrives', 'at', '180', 'degrees', 'C.', '\\n'])\n",
      "(['DG', ':', 'Ich', 'finde', ',', 'eine', 'der', 'größten', 'Geschichten', ',', 'die', 'wir', 'gerade', 'auf', 'dem', 'Meeresgrund', 'entdecken', ',', 'ist', ',', 'dass', 'das', 'Erste', ',', 'das', 'wir', 'nach', 'einem', 'Vulkanausbruch', 'am', 'Meeresboden', 'wieder', 'finden', ',', 'Bakterien', 'sind', '.', '\\n'], ['DG', ':', 'I', 'think', 'that', \"'s\", 'one', 'of', 'the', 'greatest', 'stories', 'right', 'now', 'that', 'we', \"'re\", 'seeing', 'from', 'the', 'bottom', 'of', 'the', 'sea', ',', 'is', 'that', 'the', 'first', 'thing', 'we', 'see', 'coming', 'out', 'of', 'the', 'sea', 'floor', 'after', 'a', 'volcanic', 'eruption', 'is', 'bacteria', '.', '\\n'])\n",
      "(['Lange', 'haben', 'wir', 'uns', 'gefragt', ':', 'Wie', 'sind', 'die', 'alle', 'da', 'unten', 'hingekommen', '?', '\\n'], ['And', 'we', 'started', 'to', 'wonder', 'for', 'a', 'long', 'time', ',', 'how', 'did', 'it', 'all', 'get', 'down', 'there', '?', '\\n'])\n",
      "(['Inzwischen', 'haben', 'wir', 'herausgefunden', ',', 'dass', 'sie', 'wahrscheinlich', 'aus', 'dem', 'Erdinneren', 'gekommen', 'sind', '.', '\\n'], ['What', 'we', 'find', 'out', 'now', 'is', 'that', 'it', \"'s\", 'probably', 'coming', 'from', 'inside', 'the', 'Earth', '.', '\\n'])\n",
      "(['Sie', 'kommen', 'nicht', 'nur', 'aus', 'der', 'Erde', '–', 'vom', 'Vulkanismus', 'angestoßene', 'Biogenese', '–', 'Bakterien', 'unterstützen', 'auch', 'diese', 'Kolonien', 'des', 'Lebens', '.', '\\n'], ['Not', 'only', 'is', 'it', 'coming', 'out', 'of', 'the', 'Earth', '--', 'so', ',', 'biogenesis', 'made', 'from', 'volcanic', 'activity', '--', 'but', 'that', 'bacteria', 'supports', 'these', 'colonies', 'of', 'life', '.', '\\n'])\n",
      "(['Der', 'Druck', 'beträgt', 'hier', '800', 'kg', 'pro', 'Quadratzentimeter', '.', '\\n'], ['The', 'pressure', 'here', 'is', '4,000', 'pounds', 'per', 'square', 'inch', '.', '\\n'])\n",
      "(['2,5', 'bis', '3', 'oder', '5', 'Kilometer', 'unter', 'der', 'Oberfläche', '–', 'kein', 'Sonnenstrahl', 'ist', 'jemals', 'bis', 'hierher', 'vorgedrungen', '.', '\\n'], ['A', 'mile', 'and', 'a', 'half', 'from', 'the', 'surface', 'to', 'two', 'miles', 'to', 'three', 'miles', '--', 'no', 'sun', 'has', 'ever', 'gotten', 'down', 'here', '.', '\\n'])\n",
      "(['Die', 'ganze', 'Energie', ',', 'die', 'diese', 'Lebewesen', 'antreibt', ',', 'kommt', 'aus', 'dem', 'Erdinneren', '–', 'Chemosynthese', 'also', '.', '\\n'], ['All', 'the', 'energy', 'to', 'support', 'these', 'life', 'forms', 'is', 'coming', 'from', 'inside', 'the', 'Earth', '--', 'so', ',', 'chemosynthesis', '.', '\\n'])\n",
      "(['Und', 'Sie', 'sehen', ',', 'wie', 'dicht', 'das', 'Leben', 'hier', 'ist', '.', '\\n'], ['And', 'you', 'can', 'see', 'how', 'dense', 'the', 'population', 'is', '.', '\\n'])\n",
      "(['Das', 'hier', 'sind', 'Röhrenwürmer', '.', '\\n'], ['These', 'are', 'called', 'tube', 'worms', '.', '\\n'])\n",
      "(['BL', ':', 'Diese', 'Würmer', 'haben', 'kein', 'Verdauungssystem', '.', 'Sie', 'haben', 'keinen', 'Mund', '.', '\\n'], ['BL', ':', 'These', 'worms', 'have', 'no', 'digestive', 'system', '.', 'They', 'have', 'no', 'mouth', '.', '\\n'])\n",
      "(['Aber', 'sie', 'haben', 'zwei', 'Arten', 'von', 'Kiemenstrukturen', '.', '\\n'], ['But', 'they', 'have', 'two', 'types', 'of', 'gill', 'structures', '.', '\\n'])\n",
      "(['Eine', 'für', 'das', 'Aufnehmen', 'von', 'Sauerstoff', 'aus', 'dem', 'Meerwasser', 'und', 'die', 'andere', 'beherbergt', 'diese', 'chemosynthetischen', 'Bakterien', ',', 'die', 'die', 'hydrothermale', 'Flüssigkeit', '–', 'das', 'heiße', 'Wasser', ',', 'das', 'Sie', 'aus', 'dem', 'Boden', 'schießen', 'sahen', '–', 'in', 'einfache', 'Zucker', 'verwandelt', ',', 'die', 'der', 'Röhrenwurm', 'verdauen', 'kann', '.', '\\n'], ['One', 'for', 'extracting', 'oxygen', 'out', 'of', 'the', 'deep', '-', 'sea', 'water', ',', 'another', 'one', 'which', 'houses', 'this', 'chemosynthetic', 'bacteria', ',', 'which', 'takes', 'the', 'hydrothermal', 'fluid', '--', 'that', 'hot', 'water', 'that', 'you', 'saw', 'coming', 'out', 'of', 'the', 'bottom', '--', 'and', 'converts', 'that', 'into', 'simple', 'sugars', 'that', 'the', 'tube', 'worm', 'can', 'digest', '.', '\\n'])\n",
      "(['DG', ':', 'Sie', 'können', 'sehen', '–', 'hier', 'ist', 'eine', 'Krabbe', ',', 'die', 'dort', 'unten', 'lebt', '.', '\\n'], ['DG', ':', 'You', 'can', 'see', ',', 'here', \"'s\", 'a', 'crab', 'that', 'lives', 'down', 'there', '.', '\\n'])\n",
      "(['Sie', 'hat', 'ein', 'Stück', 'dieser', 'Würmer', 'zu', 'fassen', 'gekriegt', '.', '\\n'], ['He', \"'s\", 'managed', 'to', 'grab', 'a', 'tip', 'of', 'these', 'worms', '.', '\\n'])\n",
      "(['Normalerweise', 'ziehen', 'sie', 'sich', 'zurück', ',', 'sobald', 'eine', 'Krabbe', 'sie', 'berührt', '.', '\\n'], ['Now', ',', 'they', 'normally', 'retract', 'as', 'soon', 'as', 'a', 'crab', 'touches', 'them', '.', '\\n'])\n",
      "(tensor([ 1405, 34181,    29,    38,     8,  2031,  7581,     3,    26,   129,\n",
      "         9577, 34181,     3,     4]), tensor([ 1433, 24518,    54,    91,    13,  2004, 24750,     4,    11,    89,\n",
      "         7673, 24518,     4,     3]))\n",
      "(tensor([    46,     52,     92,    172,    483,     61,      9,   1300,     10,\n",
      "        107590,    379,      3,      4]), tensor([ 18,  17,  42,  66,   6, 177,  14,  98, 482,  50,   5, 816,  88,  12,\n",
      "        459,   4,   3]))\n",
      "(tensor([   46,    32,    17,   178,     7, 15946,  2617,     7,  8697,     2,\n",
      "            5,    35,   446,   307,    54,     2,     2,     6,    13,    52,\n",
      "           92,   227,   161,   225,     3,     4]), tensor([  57,   86,  124,   98,    7,    5,  139,  693,  459,    7, 8179,   10,\n",
      "          16,  255,  112,  354,    2,    8,   17,   42,   38,   66,    6,  213,\n",
      "          14,  173,    7,   15,    4,    3]))\n",
      "(tensor([   65,   728,     8,     2,    19,     5,  8697,    72,   613,    16,\n",
      "           88, 83474,  4421,    72,    21,   221,     5, 12224,   164,   199,\n",
      "         1300,     8,     3,     4]), tensor([  62,  880,    7,    5,  417,   13,   10,    5, 8179,   26,  144,  489,\n",
      "          15,   16, 2959,   41, 1038,    7,  838,  911, 2497,   26,   15,   16,\n",
      "          38,    5,  139,  969,  212,   50,    5,  816,    4,    3]))\n",
      "(tensor([  26,  198,    2,    9,  195,    8,    2,   19,   13,    9, 1300,   34,\n",
      "          11, 2632,  454,    3,    4]), tensor([  18,    5,  204,    2,   11,   75,    2,   13,   10,   17,  143,    5,\n",
      "         644,   23, 3360,    4,    3]))\n",
      "(tensor([  85,   35,  179, 3273,    2,   93,    5, 2325, 2773,  346,   56,  415,\n",
      "          55,    3,    4]), tensor([ 232,   14,   75,   35,   15,    2,    5, 1742,   22, 2842,  174,    7,\n",
      "           5,  402,    4,    3]))\n",
      "(tensor([ 141, 1603,    7,  362,    8, 6515,    3,    4]), tensor([1029,    7,    5,  402,   13,  644,  231,    4,    3]))\n",
      "(tensor([  65, 2295, 2880,    8,  251,  859, 1465,    3,    4]), tensor([  62,  714, 3478,   13,   35,  117,  733,    4,    3]))\n",
      "(tensor([  197,   200,    56,  2951,     8,     2,    19,    13,   110,  2983,\n",
      "          436,    53,   500,    30,   103,    69,    81,     6,    27,     5,\n",
      "         1088,  2428, 10573,   554,     2,     6,    16, 40332,     6,   904,\n",
      "           28,     2,    14,    73,  3046,     2, 27698,     6, 16316,     2,\n",
      "           57,    13,    32,   108,  1279,     2,    44,   469,  7505,     8,\n",
      "            3,     4]), tensor([ 6006,     7,     5,   204,     2,    11,    75,     2,    13,    17,\n",
      "          777,    39,     5,  2772,     2,    55,    17,    76,   744,    46,\n",
      "           19,     7,     5,   644,     2,     8,    14,   128,    63,    39,\n",
      "           19,   182,   195,   676, 16166,     2,     8,    15,    16, 27856,\n",
      "            8,    15,    16,   585,     8,    43,    16,  2168,     8,    43,\n",
      "           16,  9252,     8,    43,    16, 10404,     2,    51,    14,    24,\n",
      "          119,   184,    23,    33,  2251,    12,    43,     4,     3]))\n",
      "(tensor([  138,    24,  5937,  1037,    28,     5, 13071, 28249,    56,   415,\n",
      "            3,     4]), tensor([   18,    12,     5,  1742,     2,    43,    22,     5,  5839,  1453,\n",
      "        10347,    27,     5,   402,     4,     3]))\n"
     ]
    }
   ],
   "source": [
    "# vocab allows us see which index maps to which word\n",
    "de_vocab, en_vocab = train_dataset.get_vocab()\n",
    "\n",
    "def print_top(n_rows, mode='words'):\n",
    "    if mode == 'words':\n",
    "        for i in range(n_rows):\n",
    "            de_sentence = [de_vocab.itos[index] for index in train_dataset[i][0]]\n",
    "            en_sentence = [en_vocab.itos[index] for index in train_dataset[i][1]]\n",
    "            print((de_sentence, en_sentence))\n",
    "    elif mode == 'indices':\n",
    "        for i in range(n_rows):\n",
    "            print(train_dataset[i])\n",
    "\n",
    "print_top(100, 'words')\n",
    "print_top(10, 'indices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58423\n",
      "58423\n",
      "133406\n",
      "133406\n"
     ]
    }
   ],
   "source": [
    "print(len(en_vocab.itos))\n",
    "print(len(en_vocab.stoi))\n",
    "print(len(de_vocab.itos))\n",
    "print(len(de_vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(x, vocab_size, d_model=512):\n",
    "    # two embeddings (1) each token's numeral value is mapped to a embedding vector, index (scalar) -> embedding vector (size of d_model) \n",
    "    # (2) positional embedding is applied\n",
    "    number_to_embedding = nn.Embedding(vocab_size, d_model)\n",
    "    pos_embedding = PositionalEncoding(d_model)\n",
    "    x = number_to_embedding(x) * math.sqrt(d_model)\n",
    "    x = pos_embedding(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad_value=0):\n",
    "        src_vocab_size, trg_vocab_size = len(de_vocab.itos), len(en_vocab.itos)\n",
    "        # input src & trg are shape (sentence_len, batch_size) and embedded into (sentence_len, batch_size, d_model)\n",
    "        self.src = embed(src, src_vocab_size)\n",
    "        if trg != None:\n",
    "            # given src & trg_x, we try to predict trg_y, which has ntokens words (i.e. we make ntokens predictions)\n",
    "            trg_embedding = embed(trg, trg_vocab_size)\n",
    "            self.trg_x = trg_embedding[:-1, :, :]\n",
    "            self._trg_y = trg[1:, :]\n",
    "            self.trg_y = self._trg_y.reshape(-1)\n",
    "            self.ntokens = (self._trg_y != pad_value).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tells dataloader how you want your batch to look like \n",
    "# input is a list of tensors of size batch_size (dataloader just feeds you a mini-batch of batch_size at a time and you can process it),\n",
    "# where each tensor is [src, trg]\n",
    "# output is whatever you want in train_epoch\n",
    "def collate_batch(batch_data, pad_idx=1):\n",
    "    max_src_len = max([len(sentence_pair[0]) for sentence_pair in batch_data])\n",
    "    max_trg_len = max([len(sentence_pair[1]) for sentence_pair in batch_data])\n",
    "    # initialize the padding in the shape of the result src/trg we want\n",
    "    res_src = torch.zeros(len(batch_data), max_src_len).long() + pad_idx\n",
    "    res_trg = torch.zeros(len(batch_data), max_trg_len).long() + pad_idx\n",
    "    # layer the actual sentence on top of the padding\n",
    "    for i, sentence_pair in enumerate(batch_data):\n",
    "        src_sentence, trg_sentence = sentence_pair        \n",
    "        res_src[i, :len(src_sentence):], res_trg[i, :len(trg_sentence):] = src_sentence.long(), trg_sentence.long() # the first part of sentence are filled with words, the rest are pads\n",
    "        \n",
    "    return Batch(res_src, res_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into batches, using Dataloader\n",
    "sorted_train_dataset = sorted(train_dataset, key=lambda x: (len(x[0]), len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output from a dataloader must be of shape u\n",
    "dataloader = DataLoader(sorted_train_dataset, batch_size=16, shuffle=False, collate_fn=lambda b: collate_batch(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fc0cea47e90>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "## batch.src:  tensor([[[ 36.3410,  -5.3847,   0.0000,  ..., -15.5931,   0.0000,  22.1422],\n",
      "         [-12.9151,  -3.6408, -27.6591,  ...,   0.0000,   5.7168,   3.4524],\n",
      "         [  3.6409, -10.4176,  22.5471,  ...,   3.1294,   6.3004, -14.7257]],\n",
      "\n",
      "        [[-37.4013, -13.9052,  -0.0000,  ...,  29.8335,  15.2723,  16.2829],\n",
      "         [-11.9801,  -4.1515,  -0.0000,  ...,   8.5213,   5.7169,   3.4524],\n",
      "         [  4.5759,  -0.0000,  23.4603,  ...,   0.0000,   6.3005, -14.7257]],\n",
      "\n",
      "        [[ 45.1240,  -4.1163,  56.4353,  ...,  -9.1390,   0.2787,  29.9507],\n",
      "         [-11.9048,  -5.2143, -26.6186,  ...,   8.5213,   5.7170,   3.4524],\n",
      "         [  0.0000, -11.9911,   0.0000,  ...,   3.1294,   0.0000,  -0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0.7399,   9.6344,   0.0000,  ...,  29.2805, -16.5202,  18.6147],\n",
      "         [ 33.7402,  -0.7952,  17.9800,  ..., -30.9216, -36.8936, -29.0791],\n",
      "         [-12.4482,  -3.7436, -27.6877,  ...,   8.5213,   5.7183,   3.4524]],\n",
      "\n",
      "        [[ 26.8298, -15.5965, -29.3601,  ...,  15.2011, -12.5817,  -0.0000],\n",
      "         [ -1.8532,  -7.4373, -24.0454,  ..., -61.4603, -16.1864,  -1.2987],\n",
      "         [-11.8144,  -4.5999, -26.7625,  ...,   8.5213,   5.7184,   3.4524]],\n",
      "\n",
      "        [[ -0.0000,  11.6372,   9.9593,  ...,  -9.6222,   2.3779,  -3.4378],\n",
      "         [ -2.2314,  -8.4333, -23.8918,  ..., -61.4603, -16.1863,  -1.2987],\n",
      "         [-12.1925,  -5.5960, -26.6089,  ...,   0.0000,   5.7185,   3.4524]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "## batch.trg:  tensor([[[ 4.4846e+01, -2.1508e+00, -6.1311e+00,  ...,  3.1907e+01,\n",
      "           4.7098e+00, -0.0000e+00],\n",
      "         [ 3.8900e+01,  2.9543e+01,  1.2940e+01,  ..., -2.4103e+01,\n",
      "          -1.3787e+01, -5.1996e+01],\n",
      "         [ 7.7018e+00, -1.1193e+01,  8.5911e+00,  ..., -7.0564e+00,\n",
      "          -3.7762e+01, -6.2657e+01],\n",
      "         ...,\n",
      "         [-0.0000e+00, -4.6047e+01,  0.0000e+00,  ...,  1.5467e+01,\n",
      "           2.7779e+01, -4.9695e+00],\n",
      "         [-2.0989e+01, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           2.7779e+01, -4.9695e+00],\n",
      "         [-2.0989e+01, -4.6047e+01,  4.4953e+00,  ...,  0.0000e+00,\n",
      "           2.7779e+01, -4.9695e+00]],\n",
      "\n",
      "        [[ 2.2877e+01,  3.0483e+01,  3.4047e+01,  ...,  2.4448e+01,\n",
      "           2.6699e+01, -8.1489e+00],\n",
      "         [ 3.5176e+01, -0.0000e+00,  2.8675e+01,  ..., -4.0421e+01,\n",
      "           9.5882e+00, -2.0616e+00],\n",
      "         [ 8.6368e+00, -1.1703e+01,  9.5043e+00,  ..., -7.0564e+00,\n",
      "          -3.7762e+01, -6.2657e+01],\n",
      "         ...,\n",
      "         [-0.0000e+00, -4.6558e+01,  5.4084e+00,  ...,  1.5467e+01,\n",
      "           2.7779e+01, -0.0000e+00],\n",
      "         [-2.0054e+01, -4.6558e+01,  0.0000e+00,  ...,  1.5467e+01,\n",
      "           2.7779e+01, -4.9695e+00],\n",
      "         [-2.0054e+01, -4.6558e+01,  5.4084e+00,  ...,  1.5467e+01,\n",
      "           2.7779e+01, -4.9695e+00]],\n",
      "\n",
      "        [[ 2.3906e+01, -1.2438e+01,  6.4147e+00,  ...,  2.8891e-02,\n",
      "           5.5512e+00, -1.7171e+01],\n",
      "         [ 1.4462e+01,  3.0881e+01,  1.3743e+01,  ...,  1.6305e+01,\n",
      "          -4.7880e+01,  3.5338e+01],\n",
      "         [ 1.6878e+00,  1.3637e+01, -3.0311e+01,  ..., -2.4116e+01,\n",
      "          -1.2029e+01,  4.0020e+01],\n",
      "         ...,\n",
      "         [-1.9979e+01, -0.0000e+00,  5.5357e+00,  ...,  1.5467e+01,\n",
      "           2.7780e+01, -4.9695e+00],\n",
      "         [-1.9979e+01, -4.7621e+01,  5.5357e+00,  ...,  1.5467e+01,\n",
      "           2.7780e+01, -4.9695e+00],\n",
      "         [-1.9979e+01, -4.7621e+01,  5.5357e+00,  ...,  1.5467e+01,\n",
      "           2.7780e+01, -4.9695e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.7447e+01,  8.3846e+00,  8.5349e+00,  ..., -0.0000e+00,\n",
      "          -5.2890e+00,  2.2899e+01],\n",
      "         [ 7.1056e+00, -1.1366e+01,  7.6619e+00,  ..., -0.0000e+00,\n",
      "          -3.7761e+01, -6.2657e+01],\n",
      "         [ 6.4829e+00,  3.8652e+00, -3.9386e+01,  ...,  2.3728e+01,\n",
      "          -0.0000e+00,  3.8301e+00],\n",
      "         ...,\n",
      "         [-2.1585e+01, -4.6221e+01,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           2.7781e+01, -4.9695e+00],\n",
      "         [-2.1585e+01, -4.6221e+01,  3.5661e+00,  ...,  0.0000e+00,\n",
      "           2.7781e+01, -4.9695e+00],\n",
      "         [-0.0000e+00, -4.6221e+01,  3.5661e+00,  ...,  1.5467e+01,\n",
      "           2.7781e+01, -0.0000e+00]],\n",
      "\n",
      "        [[-1.6369e+01,  7.6465e+00, -1.5501e+01,  ...,  4.1991e+01,\n",
      "           2.4007e+01,  6.6576e+00],\n",
      "         [ 2.0269e+01,  1.4342e+00,  1.2130e+01,  ...,  8.0064e+00,\n",
      "          -3.4358e+01,  1.1829e+01],\n",
      "         [ 7.5459e+00,  3.9359e+00, -3.8485e+01,  ...,  2.3728e+01,\n",
      "          -2.0736e+01,  3.8301e+00],\n",
      "         ...,\n",
      "         [-0.0000e+00, -4.6150e+01,  4.4666e+00,  ...,  1.5467e+01,\n",
      "           2.7781e+01, -4.9695e+00],\n",
      "         [-2.0522e+01, -4.6150e+01,  4.4666e+00,  ...,  1.5467e+01,\n",
      "           2.7781e+01, -4.9695e+00],\n",
      "         [-2.0522e+01, -4.6150e+01,  4.4666e+00,  ...,  1.5467e+01,\n",
      "           2.7781e+01, -4.9695e+00]],\n",
      "\n",
      "        [[-3.4981e+01, -2.0012e+01,  3.6802e+01,  ...,  2.2457e+01,\n",
      "           5.7303e+01, -3.7584e+01],\n",
      "         [ 8.8025e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "          -3.7760e+01, -6.2657e+01],\n",
      "         [ 0.0000e+00,  3.0795e+00, -3.7560e+01,  ...,  0.0000e+00,\n",
      "          -2.0736e+01,  3.8301e+00],\n",
      "         ...,\n",
      "         [-1.9888e+01, -4.7007e+01,  5.3918e+00,  ...,  1.5467e+01,\n",
      "           2.7781e+01, -4.9695e+00],\n",
      "         [-1.9888e+01, -4.7007e+01,  0.0000e+00,  ...,  1.5467e+01,\n",
      "           2.7781e+01, -4.9695e+00],\n",
      "         [-1.9888e+01, -4.7007e+01,  5.3918e+00,  ...,  1.5467e+01,\n",
      "           2.7781e+01, -4.9695e+00]]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    if i > 0:\n",
    "        print(\"## batch.src: \", batch.src)\n",
    "        print(\"## batch.trg: \", batch.trg_x)\n",
    "        break\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(batch, model, hyper_params):\n",
    "    def forward_pass():\n",
    "        src = batch.src.to(device=device)\n",
    "        trg_x = batch.trg_x.to(device=device)\n",
    "        pred = model(src, trg_x)\n",
    "        return pred\n",
    "    \n",
    "    def calculate_loss(pred):\n",
    "        trg_y = batch.trg_y.to(device=device)\n",
    "        loss = hyper_params.criterion(pred.view(-1,tgt_vocab_size), trg_y)  \n",
    "        return loss\n",
    "\n",
    "    pred = foward_pass()\n",
    "    loss = calculate_loss(pred)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every epoch is an iteration over the entire training set (how many steps are in one epoch depends on the batch_size)\n",
    "def train_epoch(data, model, hyper_params):\n",
    "    model.train() \n",
    "    # varialbes for logging\n",
    "    log = Log()\n",
    "    # train the model batch-by-batch \n",
    "    for i, batch in enumerate(data):\n",
    "        batch_loss = train_batch(batch, model, hyper_params)\n",
    "        log.batch_info(batch, batch_loss)\n",
    "        if i % 50 == 1:\n",
    "            log.show_every_50batches(i)\n",
    "    log.show_epoch()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "pad_value = TRG.vocab.stoi['<pad>']\n",
    "d_model = 512\n",
    "lr = 5.0 \n",
    "\n",
    "# initialize the training params\n",
    "# data = (Batch(batch.src, batch.trg, pad_value) for batch in data_loader)\n",
    "\n",
    "# initialize model\n",
    "encoder_decoder = nn.Transformer().to(device)\n",
    "generator = Generator(d_model, tgt_vocab_size).to(device)\n",
    "model = Model(encoder_decoder, generator)\n",
    "\n",
    "hyper_params = HyperParams(criterion = nn.CrossEntropyLoss(), optimizer = torch.optim.SGD(model.parameters(), lr=lr), scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for 10 epochs\n",
    "for epoch in range(10):\n",
    "    print(\"------------Training epoch \", epoch, \"--------------\")\n",
    "    train_epoch(data, model, hyper_params)\n",
    "  # set to eval model to check how good the model is after each loop\n",
    "  # print(train_epoch((batchify(b, pad_idx, device) for b in valid_iterator), model, criterion, epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-transformer",
   "language": "python",
   "name": "test-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
