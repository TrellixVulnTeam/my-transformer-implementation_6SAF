{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertEqual(t1, t2, message=\"\"):\n",
    "    assert torch.eq(t1, t2).all()\n",
    "    print(\"Checking: \", message, \"     ...OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_head corresponds to d_k in the paper\n",
    "def scaled_dot_product_attention(value, key, query, dropout=0.0):\n",
    "    \"\"\"\n",
    "    Shape:\n",
    "    - Inputs:\n",
    "    - query: `(..., T, N * H, E / H)`\n",
    "    - key: `(..., S, N * H, E / H)`\n",
    "    - value: `(..., S, N * H, E /H)`\n",
    "    \n",
    "    where E = d_model, E/H = d_head\n",
    "    \n",
    "    - Outputs:\n",
    "    - `(..., T, N * H, E / H)`, `(N * H, T, S)`\n",
    "    \"\"\"\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"The d_head of query, key, value must be equal.\"\n",
    "    S, T, N_H, d_head = key.shape[-3], query.shape[-3], query.shape[-2], query.shape[-1]\n",
    "    \n",
    "    query, key, value = query.transpose(-2, -3), key.transpose(-2, -3), value.transpose(-2, -3)\n",
    "\n",
    "    # calculates attention weights\n",
    "    query = query * (float(d_head) ** -0.5)     \n",
    "    attention_weights = torch.matmul(query, key.transpose(-2,-1))\n",
    "    attention_weights = torch.nn.functional.softmax(attention_weights, dim=-1)\n",
    "    attention_weights = torch.nn.functional.dropout(attention_weights, p=dropout)\n",
    "    assert attention_weights.shape == (N_H, T, S), \"attention_weights should be shape (N * H, T, S)\"\n",
    "\n",
    "    attention_output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return attention_output.transpose(-3, -2), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:  attention_weights are expected?      ...OK!\n",
      "Checking:  attention_output is expected?      ...OK!\n"
     ]
    }
   ],
   "source": [
    "def test_sdp():\n",
    "    q = torch.randn(25, 256, 3)\n",
    "    k = v = torch.randn(21, 256, 3)\n",
    "    \n",
    "    # call torchtext's SDP\n",
    "    SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)\n",
    "    expected_attn_output, expected_attn_weights = SDP(q, k, v)\n",
    "    \n",
    "    # call our SDP\n",
    "    torch.manual_seed(42)    \n",
    "    attn_output, attn_weights = scaled_dot_product_attention(v, k, q, dropout=0.1) \n",
    "    \n",
    "    assert attn_weights.shape == expected_attn_weights.shape\n",
    "    assertEqual(attn_weights, expected_attn_weights, message = \"attention_weights are expected?\")\n",
    "    assert attn_output.shape == expected_attn_output.shape, \"attn_output.shape is {0} whereas expected_output.shape is {1}\".format(attn_output.shape, expected_attn_output.shape) \n",
    "    assertEqual(attn_output, expected_attn_output, message = \"attention_output is expected?\")\n",
    "    \n",
    "test_sdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.linears = _get_clones(linear, 3)\n",
    "        \n",
    "        \n",
    "    def forward(self, attended, attending):\n",
    "        # input dimension is (sentence_len, batch_size, d_model)\n",
    "        W_v, W_k, W_q = self.linears\n",
    "\n",
    "        value = W_v(attended)\n",
    "        key = W_k(attended)\n",
    "        query = W_q(attending)\n",
    "\n",
    "        return value, key, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:  query is expected?      ...OK!\n",
      "Checking:  key is expected?      ...OK!\n",
      "Checking:  value is expected?      ...OK!\n"
     ]
    }
   ],
   "source": [
    "def test_projection():\n",
    "    from torchtext.nn import InProjContainer\n",
    "\n",
    "    d_model, batch_size = 10, 64\n",
    "    q = torch.rand((5, batch_size, d_model))\n",
    "    k = v = torch.rand((6, batch_size, d_model))\n",
    "\n",
    "    # call torchtext's InProjContainer class\n",
    "    torch.manual_seed(42)\n",
    "    l1 = torch.nn.Linear(d_model, d_model)\n",
    "    l2 = l3 = copy.deepcopy(l1)\n",
    "    in_proj_container = InProjContainer(l1, l2, l3)\n",
    "    expected_query, expected_key, expected_value = in_proj_container(q, k, v)\n",
    "\n",
    "    # call our Projection class\n",
    "    projection = Projection(d_model)\n",
    "    value, key, query = projection(k, q)\n",
    "\n",
    "    assert expected_query.shape == query.shape\n",
    "    assert expected_key.shape == key.shape\n",
    "    assert expected_value.shape == value.shape\n",
    "\n",
    "    assertEqual(expected_query, query, message = \"query is expected?\")\n",
    "    assertEqual(expected_key, key, message = \"key is expected?\")\n",
    "    assertEqual(expected_value, value, message = \"value is expected?\")\n",
    "\n",
    "test_projection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, nheads, d_model):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.projection = Projection(d_model)\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        self.W_o = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, attended, attending, mask=None):\n",
    "        '''\n",
    "        Shape:\n",
    "        - Inputs:\n",
    "        - attending: :math:`(..., T, N, d_model)`\n",
    "        - attended: :math:`(..., S, N, d_model)`\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(..., T, N, d_model)`\n",
    "        - attn_output_weights: :math:`(N * H, T, S)`\n",
    "        '''\n",
    "        # checks dimensions & assigns variables\n",
    "        assert attending.shape[-1] == attended.shape[-1], \"attending & attended should have the same d_model\"\n",
    "        d_model = attending.shape[-1]\n",
    "        assert d_model % self.nheads == 0, \"d_model should be divisible by number of heads\"\n",
    "        self.d_k = d_model // self.nheads  \n",
    "        assert attending.shape[-2] == attended.shape[-2], \"attending & attended should have the same batch size\"\n",
    "        self.batch_size = attending.shape[-2]\n",
    "        \n",
    "        # projects attended and attending tensors to v, k, q\n",
    "        value, key, query = self.projection(attended, attending)\n",
    "        value, key, query = self.reshape_into_nheads(value, self.nheads, self.d_k), self.reshape_into_nheads(key, self.nheads, self.d_k), self.reshape_into_nheads(query, self.nheads, self.d_k)\n",
    "                \n",
    "        # forward multi-heads through SDP\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(value, key, query)\n",
    "        assert attn_output.shape == (attending.shape[-3], self.batch_size * self.nheads, self.d_k), \"attn_output's shape from SDP should be (..., T, N * H, E / H)\"\n",
    "\n",
    "        # concats multi-heads and forward through final layer\n",
    "        attn_output = self.reshape_into_nheads(attn_output, 1, d_model)\n",
    "        attn_output = self.W_o(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "    def reshape_into_nheads(self, x, nheads, last_dim):\n",
    "        return x.reshape(-1, self.batch_size * nheads, last_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:  attn_output expected?      ...OK!\n",
      "Checking:  attn_weights expected?      ...OK!\n"
     ]
    }
   ],
   "source": [
    "def test_multihead_attention():\n",
    "    from torchtext.nn import InProjContainer, MultiheadAttentionContainer, ScaledDotProduct\n",
    "\n",
    "    d_model, num_heads, bsz = 10, 5, 64\n",
    "    query = torch.rand((21, bsz, d_model))\n",
    "    key = value = torch.rand((16, bsz, d_model))\n",
    "\n",
    "    # call torchtext's InProjContainer and then MHA\n",
    "    torch.manual_seed(42)\n",
    "    l1 = torch.nn.Linear(d_model, d_model)\n",
    "    l2 = l3 = l4 = copy.deepcopy(l1)\n",
    "    \n",
    "    in_proj_container = InProjContainer(l1, l2, l3)\n",
    "    MHA = MultiheadAttentionContainer(num_heads, \n",
    "                                      in_proj_container, \n",
    "                                      ScaledDotProduct(),\n",
    "                                      l4)\n",
    "    expected_attn_output, expected_attn_weights = MHA(query, key, value)\n",
    "    \n",
    "    # call our MHA\n",
    "    MY_MHA = MultiheadAttention(num_heads, d_model)\n",
    "    attn_output, attn_weights = MY_MHA(key, query)\n",
    "    \n",
    "    assert expected_attn_output.shape == attn_output.shape\n",
    "    assert expected_attn_weights.shape == expected_attn_weights.shape\n",
    "    assertEqual(expected_attn_output, attn_output, message = \"attn_output expected?\")\n",
    "    assertEqual(expected_attn_weights, attn_weights, message = \"attn_weights expected?\")\n",
    "\n",
    "test_multihead_attention()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
